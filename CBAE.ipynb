{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Vectorization classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Class to process text and extract vocabulary for mapping\n",
    "    \n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", unk_token=\"<UNK>\", num_token='<NUM>'):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        self._num_token = num_token\n",
    "        \n",
    "        self.unk_index = self.add_token(unk_token) \n",
    "        self.num_index = self.add_token(num_token)\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Returns a dictionary that can be serialized\"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token, \n",
    "                'mask_token': self._mask_token,\n",
    "                'num_token': self._num_token,\n",
    "               }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Instantiates the Vocabulary from a serialized dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token or the UNK index if token isn't present\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) for the UNK functionality \n",
    "\n",
    "        \"\"\"\n",
    "        if self.is_number(token):\n",
    "            return self.num_index\n",
    "            \n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def is_number(self, token):\n",
    "        \"\"\"Returns true if token in number else false\"\"\"\n",
    "        num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "        return bool(num_regex.match(token))\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\n",
    "    \n",
    "        Args:\n",
    "            vocab (Vocabulary): maps words to integers\n",
    "\n",
    "    \"\"\"    \n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"Vectorizer\n",
    "\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[-len(indices):] = indices\n",
    "        out_vector[:-len(indices)] = self.vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            df(pandas.DataFrame): the target dataset\n",
    "\n",
    "        Returns:\n",
    "            an instance of the Vectorizer\n",
    "\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary()\n",
    "        for index, row in df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                vocab.add_token(token)\n",
    "            vocab.add_token(row.target)\n",
    "            \n",
    "        return cls(vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        vocab = Vocabulary.from_serializable(contents['vocab'])\n",
    "        return cls(vocab=vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'vocab': self.vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \"\"\" Dataset reader\n",
    "\n",
    "        Args:\n",
    "            df(pandas.DataFrame): the dataset\n",
    "            vectorizer (Vectorizer): vectorizer instatiated from dataset\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df, test_df, vectorizer):\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.train_size = len(train_df)\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.test_size = len(test_df)\n",
    "        \n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, train_df.sentence))\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @property\n",
    "    def max_seq_length(self):\n",
    "        \"\"\"Max dataset sequence len\"\"\"\n",
    "        return self._max_seq_length\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"Selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Returns the vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        target = 0.\n",
    "\n",
    "        context_vector = self._vectorizer.vectorize(row.sentence, self._max_seq_length)\n",
    "\n",
    "        return {\n",
    "            'x_data': context_vector,\n",
    "            'y_target': 0. if self._target_split == 'train' else row.label,\n",
    "        }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"A generator function which wraps the PyTorch DataLoader. It will ensure \n",
    "        each tensor is on the write device location\n",
    "        \n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device) \\\n",
    "            if isinstance(data_dict[name], torch.Tensor) else data_dict[name]\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Text preprocessing regular expression\"\"\"\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for aspects matrix initialization\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output(input_, kernel, padding, stride):\n",
    "    \"\"\"Calculate the output size in Convolution layer\"\"\"\n",
    "    return math.floor(((input_ - kernel + 2 * padding) / stride) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model: CBAE(Capsule Based Aspect Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\" N-gram convolutional layer\n",
    "    \n",
    "        Args:\n",
    "            in_channels: convolutional input channels\n",
    "            out_channels: convolutional output channels\n",
    "            kernel_size: convolutional kernel size\n",
    "            stride: convolutional stride\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\" Primary caps layer\n",
    "    \n",
    "        Args:\n",
    "            num_capsules: capsules count\n",
    "            in_channels: input channels\n",
    "            out_channels: output channels\n",
    "            kernel_size: capsule kernel size\n",
    "            stride: capsule stride\n",
    "            conv_out_size: tensor size from above layer\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules, in_channels, out_channels, kernel_size, stride, conv_out_size):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=0\n",
    "            ) \n",
    "            for _ in range(num_capsules)\n",
    "        ])\n",
    "        \n",
    "        self._out_channels = out_channels\n",
    "        self._conv_out_size = conv_out_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), self._out_channels * self._conv_out_size , -1)\n",
    "\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm + 1e-07))\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryCaps(nn.Module):\n",
    "    \"\"\" Secondary capsules layer\n",
    "    \n",
    "        Args:\n",
    "            num_capsules: capsules count\n",
    "            num_routes: routing iteration count\n",
    "            in_channels: in channels dim\n",
    "            out_channels: out channels dim\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsules, num_routes, in_channels, out_channels):\n",
    "        super(SecondaryCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1)).to(args.device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm + 1e-07))\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    \"\"\" Caps net module of CBAE\n",
    "    \n",
    "        Args:\n",
    "            conv_out_ch: N-gram convolutional out channels\n",
    "            conv_kernel: N-gram convolutional kernel size\n",
    "            conv_stride: N-gram convolutional stride\n",
    "            prime_num_capsules: Primary capsules count\n",
    "            prime_out_ch: Primary capsules out channels\n",
    "            prime_kernel: Primary capsules kernel size\n",
    "            prime_stride: Primary capsules stride\n",
    "            secondary_num_capsules: Secondary capsules count\n",
    "            secondary_out_channels=Secondary capsules out channels\n",
    "            maxlen: sentence max length taken into account\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_in_ch,\n",
    "        conv_out_ch,\n",
    "        conv_kernel,\n",
    "        conv_stride,\n",
    "        prime_num_capsules,\n",
    "        prime_out_ch,\n",
    "        prime_kernel,\n",
    "        prime_stride,\n",
    "        secondary_num_capsules,\n",
    "        secondary_out_channels,\n",
    "        input_len,\n",
    "    ):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self._secondary_out_size=secondary_out_channels * secondary_num_capsules\n",
    "        \n",
    "        self.conv_layer = ConvLayer(\n",
    "            in_channels=conv_in_ch,\n",
    "            out_channels=conv_out_ch,\n",
    "            kernel_size=conv_kernel,\n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        conv_layer_output = calculate_conv_output(\n",
    "            input_=input_len, \n",
    "            kernel=conv_kernel, \n",
    "            padding=0, \n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        \n",
    "        prime_caps_conv_output = calculate_conv_output(\n",
    "            input_=conv_layer_output, \n",
    "            kernel=prime_kernel, \n",
    "            padding=0, \n",
    "            stride=prime_stride,\n",
    "        )\n",
    "        \n",
    "        self.primary_caps = PrimaryCaps(\n",
    "            num_capsules=prime_num_capsules, \n",
    "            in_channels=conv_out_ch, \n",
    "            out_channels=prime_out_ch, \n",
    "            kernel_size=prime_kernel, \n",
    "            stride=prime_stride,\n",
    "            conv_out_size=prime_caps_conv_output,\n",
    "        )\n",
    "        \n",
    "        self.secondary_caps = SecondaryCaps(\n",
    "            num_capsules=secondary_num_capsules,\n",
    "            num_routes=prime_caps_conv_output * prime_out_ch,\n",
    "            in_channels=prime_num_capsules,\n",
    "            out_channels=secondary_out_channels,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(secondary_out_channels * secondary_num_capsules, input_len)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.capsule_softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.secondary_caps(self.primary_caps(self.conv_layer(data)))\n",
    "        output = output.reshape(-1, self._secondary_out_size)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc(output)\n",
    "        return self.capsule_softmax(self.tanh(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAE(torch.nn.Module):\n",
    "    \"\"\" Capsule Based Aspect Extraction\n",
    "        \n",
    "        Args:\n",
    "            wv_dim: word vector size\n",
    "            asp_count: number of aspects\n",
    "            ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
    "            maxlen: sentence max length taken into account\n",
    "            init_aspects_matrix: None or init. matrix for aspects\n",
    "            pretrained_embedding: w2v vectors\n",
    "            padding_index: Mask index\n",
    "            cn_conv_out_ch: N-gram convolutional out channels\n",
    "            cn_conv_kernel: N-gram convolutional kernel size\n",
    "            cn_conv_stride: N-gram convolutional stride\n",
    "            cn_prime_num_capsules: Primary capsules count\n",
    "            cn_prime_out_ch: Primary capsules out channels\n",
    "            cn_prime_kernel: Primary capsules kernel size\n",
    "            cn_prime_stride: Primary capsules stride\n",
    "            cn_secondary_num_capsules: Secondary capsules count\n",
    "            cn_secondary_out_channels=Secondary capsules out channels\n",
    "            encoder_only: bool - return output after encoding\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim, \n",
    "        asp_count,\n",
    "        maxlen, \n",
    "        init_aspects_matrix,\n",
    "        pretrained_embedding,\n",
    "        padding_index,\n",
    "        cn_conv_out_ch,\n",
    "        cn_conv_kernel,\n",
    "        cn_conv_stride,\n",
    "        cn_prime_num_capsules,\n",
    "        cn_prime_out_ch,\n",
    "        cn_prime_kernel,\n",
    "        cn_prime_stride,\n",
    "        cn_secondary_num_capsules,\n",
    "        cn_secondary_out_channels,\n",
    "        encoder_only=False,\n",
    "    ):\n",
    "        super(CBAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(pretrained_embedding), \n",
    "            padding_idx=padding_index,\n",
    "        )\n",
    "        \n",
    "        self.caps_net = CapsNet(\n",
    "            conv_in_ch=wv_dim,\n",
    "            conv_out_ch=cn_conv_out_ch,\n",
    "            conv_kernel=cn_conv_kernel,\n",
    "            conv_stride=cn_conv_stride,\n",
    "            prime_num_capsules=cn_prime_num_capsules,\n",
    "            prime_out_ch=cn_prime_out_ch,\n",
    "            prime_kernel=cn_prime_kernel,\n",
    "            prime_stride=cn_prime_stride,\n",
    "            secondary_num_capsules=cn_secondary_num_capsules,\n",
    "            secondary_out_channels=cn_secondary_out_channels,\n",
    "            input_len=maxlen,\n",
    "        )\n",
    "\n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax(dim=1)\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "            \n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Get aspect importances\n",
    "        \n",
    "        Args:\n",
    "            text_embedding: embeddings of a sentence as input\n",
    "        \n",
    "        Returns: \n",
    "            capsule weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "        \"\"\"\n",
    "        # compute capsule scores, looking at text embeddings average\n",
    "        caps_weights = self.caps_net(text_embeddings.permute(0, 2, 1))\n",
    "\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(caps_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return caps_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "        \n",
    "        text_embeddings = self.embedding(text_embeddings)\n",
    "\n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "        \n",
    "        if self.encoder_only:\n",
    "            return aspects_importances\n",
    "        else:\n",
    "            negative_samples_texts = self.embedding(negative_samples_texts)\n",
    "            \n",
    "            # negative samples are averaged\n",
    "            averaged_negative_samples = torch.mean(negative_samples_texts, dim=1)\n",
    "            averaged_negative_samples = torch.mean(averaged_negative_samples, dim=1)\n",
    "            \n",
    "            # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "            recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "            \n",
    "            return weighted_text_emb, recovered_emb, averaged_negative_samples\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=10):\n",
    "        \"\"\"Getting aspects words\"\"\"\n",
    "        words = []\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "        words_scores = w2v_model.wv.vectors.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "    \n",
    "def make_train_state(args):\n",
    "    return {\n",
    "        'stop_early': False,\n",
    "        'early_stopping_step': 0,\n",
    "        'early_stopping_best_val': 1e8,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'epoch_index': 0,\n",
    "        'train_loss': [],\n",
    "        'model_filename': args.model_state_file\n",
    "    }\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    Args:\n",
    "        args: main arguments\n",
    "        model: model to train\n",
    "        train_state: a dictionary representing the training state values\n",
    "    \n",
    "    Returns:\n",
    "        new train_state\n",
    "\n",
    "    \"\"\"\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['train_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    train_data='preprocessed_data/restaurant/train.txt',\n",
    "    test_data='preprocessed_data/restaurant/test.txt',\n",
    "    test_labels='preprocessed_data/restaurant/test_label.txt',\n",
    "    emb_path='preprocessed_data/restaurant/w2v_embedding',\n",
    "    \n",
    "    emb_dim=200,\n",
    "    batch_size=50,\n",
    "    vocab_size=9000,\n",
    "    aspect_size=14,\n",
    "    epochs=10,\n",
    "    neg_size=20,\n",
    "    maxlen=-1,\n",
    "\n",
    "    cn_conv_out_channels = 200,\n",
    "    cn_conv_kernel = 3,\n",
    "    cn_conv_stride = 1,\n",
    "    cn_prime_num_capsules=7,\n",
    "    cn_prime_kernel=3,\n",
    "    cn_prime_out_channels=32,\n",
    "    cn_prime_stride=1,\n",
    "    cn_secondary_num_capsules=7,\n",
    "    cn_secondary_out_channels=32,\n",
    "\n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    "    learning_rate=1e-3,\n",
    "    early_stopping_criteria=5,  \n",
    "    catch_keyboard_interrupt=True,\n",
    "    seed=1234,\n",
    "    \n",
    "    save_dir=\"model_storage\",\n",
    "    model_state_file=\"model.pth\",\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bb9cee1f3043afa477b96e206c33a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=279885.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "279885 sentences\n",
      "Sample: place fancy wouldn go date \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like jeollado like roll sometimes price variet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like roll tiny order anyway often get order wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money dependable fun place get sushi bring fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place great deal price food give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crab roll made real crab imitation crab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  like jeollado like roll sometimes price variet...\n",
       "1  like roll tiny order anyway often get order wr...\n",
       "2  money dependable fun place get sushi bring fri...\n",
       "3                  place great deal price food give \n",
       "4           crab roll made real crab imitation crab "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with open('preprocessed_data/restaurant/train.txt') as fp:\n",
    "    for line in tqdm(fp.readlines()):\n",
    "        sentences.append(line)\n",
    "        \n",
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "print (len(cleaned_sentences), \"sentences\")\n",
    "print (\"Sample:\", cleaned_sentences[42])\n",
    "\n",
    "train_df = pd.DataFrame(cleaned_sentences, columns=[\"sentence\"])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa903472d9ac4b7d89155011ab7b10bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86f2e6841c44567a2c4407b18d72fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1490 sentences\n",
      "Sample sentence: food prepared quickly efficiently \n",
      "Sample label: food\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>staff friendliest competent stickler service e...</td>\n",
       "      <td>staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great group great date great early brunch nigh...</td>\n",
       "      <td>ambience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like cafe noir dont get wrong jsut people work...</td>\n",
       "      <td>staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>service terrible wait everything ask several d...</td>\n",
       "      <td>staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>waitress seems concerned looking good actually...</td>\n",
       "      <td>staff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label\n",
       "0  staff friendliest competent stickler service e...     staff\n",
       "1  great group great date great early brunch nigh...  ambience\n",
       "2  like cafe noir dont get wrong jsut people work...     staff\n",
       "3  service terrible wait everything ask several d...     staff\n",
       "4  waitress seems concerned looking good actually...     staff"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "with open('preprocessed_data/restaurant/test.txt') as fp:\n",
    "    for line in tqdm(fp.readlines()):\n",
    "        sentences.append(line)\n",
    "        \n",
    "with open('preprocessed_data/restaurant/test_label.txt') as fp:\n",
    "    for line in tqdm(fp.readlines()):\n",
    "        labels.append(line)\n",
    "        \n",
    "cleaned_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "cleaned_labels = [preprocess_text(label.split()[0]) for label in labels]\n",
    "\n",
    "print (len(cleaned_sentences), \"sentences\")\n",
    "print (\"Sample sentence:\", cleaned_sentences[42])\n",
    "print (\"Sample label:\", cleaned_labels[42])\n",
    "    \n",
    "test_df = pd.DataFrame({'sentence': cleaned_sentences, 'label': cleaned_labels})\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load(args.emb_path)\n",
    "token2index_lim = {token: index for index, token in enumerate(w2v.wv.index2word) if index < args.vocab_size}\n",
    "token2index_all = {token: index for index, token in enumerate(w2v.wv.index2word)}\n",
    "vocab = Vocabulary(token2index_lim)\n",
    "vectorizer = Vectorizer(vocab)\n",
    "dataset = Dataset(train_df, test_df , vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBAE(\n",
    "    wv_dim=args.emb_dim,\n",
    "    asp_count=args.aspect_size,\n",
    "    maxlen=dataset.max_seq_length, \n",
    "    init_aspects_matrix=get_centroids(w2v, args.aspect_size),\n",
    "    pretrained_embedding=w2v.wv.vectors,\n",
    "    padding_index=vocab.mask_index,\n",
    "    cn_conv_out_ch=args.cn_conv_out_channels,\n",
    "    cn_conv_kernel=args.cn_conv_kernel,\n",
    "    cn_conv_stride=args.cn_conv_stride,\n",
    "    cn_prime_num_capsules=args.cn_prime_num_capsules,\n",
    "    cn_prime_out_ch=args.cn_prime_out_channels,\n",
    "    cn_prime_kernel=args.cn_prime_kernel,\n",
    "    cn_prime_stride=args.cn_prime_stride,\n",
    "    cn_secondary_num_capsules=args.cn_secondary_num_capsules,\n",
    "    cn_secondary_out_channels=args.cn_secondary_out_channels,\n",
    ")\n",
    "\n",
    "model = model.to(args.device)\n",
    "loss_func = nn.TripletMarginLoss(margin=1, swap=False, reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode='min', \n",
    "    factor=0.5,\n",
    "    patience=1\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e611712cf5b4e9186ecb78c68745ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=10.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c049495fd8904247827dcab8a4e1940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=5597.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_bar = tqdm(\n",
    "    desc='training routine', \n",
    "    total=args.epochs,\n",
    "    position=1,\n",
    ")\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(\n",
    "    desc='train',\n",
    "    total=dataset.get_num_batches(args.batch_size), \n",
    "    position=1, \n",
    ")\n",
    "\n",
    "for epoch_index in range(args.epochs):\n",
    "    \n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    batch_generator = generate_batches(\n",
    "        dataset, \n",
    "        batch_size=args.batch_size, \n",
    "        device=args.device\n",
    "    )\n",
    "    \n",
    "    neg_batch_generator = generate_batches(\n",
    "        dataset, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=False,\n",
    "        device=args.device,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = batch_dict['x_data']\n",
    "        y = batch_dict['y_target'].float()\n",
    "        x_neg = next(neg_batch_generator)['x_data']\n",
    "\n",
    "        negative_samples = torch.stack(\n",
    "            tuple([x_neg[torch.randperm(x_neg.shape[0])[:args.neg_size]] \n",
    "                   for _ in range(args.batch_size)])\n",
    "        ).to(args.device)\n",
    "\n",
    "        anchor, positive, negative = model(x, negative_samples)\n",
    "        loss = loss_func(anchor, positive, negative)\n",
    "        loss_t = loss.item()\n",
    "\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bar.set_postfix(loss=running_loss, epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state = update_train_state(args=args, model=model, train_state=train_state)\n",
    "    scheduler.step(train_state['train_loss'][-1])\n",
    "    \n",
    "    # uncomment the lines below to display the aspects words after each training loop\n",
    "    # print(batch_index, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
    "    # for i, aspect in enumerate(model.get_aspect_words(w2v)):\n",
    "    #     print(i, \" \".join([a for a in aspect]))\n",
    "    # print()\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n",
    "    train_bar.n = 0\n",
    "\n",
    "    epoch_bar.set_postfix(best_val=train_state['early_stopping_best_val'])\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAGbCAYAAAD3MIVlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfZyddX3n/9dn7jIJSSAkQ3ImE0hmCHdFQA2gpaLVoqhb+PlzV6E/FfxVcF3RWnbdhXXXuu5222pX223ZKiIW2yIiVTdVBFmrxSpgooKQBBDDTSY3ZHJHyP3cfPeP6ww5mUwyM5lz5jo3r+fjcR7nnOu6zsznMmDefK/P9TmRUkKSJEnl1ZR3AZIkSfXIkCVJklQBhixJkqQKMGRJkiRVgCFLkiSpAlryLmCkefPmpcWLF+ddhiRJ0ph++tOfbkkpdYy2r+pC1uLFi1m5cmXeZUiSJI0pIp490j4vF0qSJFWAIUuSJKkCDFmSJEkVUHU9WZIkqXb19/fT29vLvn378i6lrNrb2+nq6qK1tXXcnzFkSZKksunt7WXWrFksXryYiMi7nLJIKbF161Z6e3tZsmTJuD/n5UJJklQ2+/btY+7cuXUTsAAigrlz5054dW5cISsiLo2IJyLiqYi44QjHvCMiVkfEqoi4vbjtlIj4WUQ8XNz+rydUnSRJqjn1FLCGHcs5jXm5MCKagZuAS4BeYEVELE8prS45ZilwI3BRSml7RJxU3LUReHVKaX9EzAQeK352w4QrlSRJqiHjWcm6AHgqpbQ2pXQAuAO4fMQx1wA3pZS2A6SUNhefD6SU9hePmTbO3ydJknTMZs6cmXcJwPhCz0JgXcn73uK2UqcBp0XEjyLiwYi4dHhHRCyKiF8Uf8afjLaKFRHXRsTKiFjZ19c38bOQJEmqMuVaWWoBlgKvA64EvhARJwCklNallM4BTgWuioj5Iz+cUro5pbQspbSso2PUr/+RJEmakJQSH/3oRzn77LN52ctexle/+lUANm7cyMUXX8x5553H2WefzQ9/+EMGBwe5+uqrXzr2s5/97KR//3hGOKwHFpW87ypuK9ULPJRS6geejognyULXiuEDUkobIuIx4DXAXZOqWpIkVb/vfBE2PV3en7lgCbz5d8d16Ne//nUefvhhHnnkEbZs2cL555/PxRdfzO23386b3vQmPvaxjzE4OMiePXt4+OGHWb9+PY899hgAO3bsmHSp41nJWgEsjYglEdEGXAEsH3HMN8lWsYiIeWSXD9dGRFdETC9unwP8BvDEpKuWJEkawz//8z9z5ZVX0tzczPz583nta1/LihUrOP/88/nSl77EJz7xCR599FFmzZpFd3c3a9eu5UMf+hD33HMPs2fPnvTvH3MlK6U0EBHXAfcCzcCtKaVVEfFJYGVKaXlx3xsjYjUwCHw0pbQ1Ii4B/kdEJCCAP00pPTrpqiVJUvUb54rTVLv44ou5//77+fa3v83VV1/N9ddfz3ve8x4eeeQR7r33Xj73uc9x5513cuutt07q94xr4ntK6W7g7hHbPl7yOgHXFx+lx9wHnDOpCmtdSrCj2Mw/56SjHytJksrmNa95DZ///Oe56qqr2LZtG/fffz+f/vSnefbZZ+nq6uKaa65h//79/OxnP+Mtb3kLbW1tvP3tb+f000/nXe9616R/v1+rMxU+fz382kXw2x/IuxJJkhrG2972Nh544AHOPfdcIoJPfepTLFiwgNtuu41Pf/rTtLa2MnPmTL785S+zfv163vve9zI0NATAH/3RH03690e2CFU9li1bllauXJl3GeV128dh/1649tN5VyJJUkWtWbOGM888M+8yKmK0c4uIn6aUlo12vMNBp0KhB55/Bgb6865EkiRNEUPWVCh0w+AA9PXmXYkkSZoihqyp0NmTPW/8Vb51SJI0BaqtFakcjuWcDFlTYc4CaJsOGwxZkqT61t7eztatW+sqaKWU2Lp1K+3t7RP6nHcXToWmpuyS4ca1eVciSVJFdXV10dvbS719F3F7eztdXV0T+owha6p09sCKe2BwEJqb865GkqSKaG1tZcmSJXmXURW8XDhVCt0wcAC22PwuSVIjMGRNlUKx+d2+LEmSGoIha6rMLUBru3cYSpLUIAxZU6WpGQpLbH6XJKlBGLKmUqEHNj0NQ4N5VyJJkirMkDWVCt3Qvx+2bMi7EkmSVGGGrKnk5HdJkhqGIWsqzV0ILW3eYShJUgMwZE2l5mZYYPO7JEmNwJA11Tp7YNNaGBrKuxJJklRBhqypVuiGA/tgq83vkiTVM0PWVBue/O4lQ0mS6poha6p1dEFzq3cYSpJU5wxZU625BRYsdiVLkqQ6Z8jKQ6EnC1k2v0uSVLcMWXkodMP+PbB9U96VSJKkCjFk5aHT5ndJkuqdISsPHYuy3iwnv0uSVLcMWXloaYWTTnElS5KkOmbIyktnsfk9pbwrkSRJFWDIykuhG/btgu3P512JJEmqAENWXpz8LklSXTNk5WX+KdDU7OR3SZLqlCErLy2tcNLJrmRJklSnDFl5KvRkYxxsfpckqe4YsvJU6Ia9L8ILfXlXIkmSysyQlScnv0uSVLcMWXmafwpEk5PfJUmqQ4asPLVOy75ix5UsSZLqjiErb5092RgHm98lSaorhqy8Fbph9wuwc2velUiSpDIyZOXNye+SJNUlQ1beFizOmt+d/C5JUl0xZOWtrR3mLfQOQ0mS6owhqxoUerxcKElSnTFkVYNCN+zaDi9uy7sSSZJUJuMKWRFxaUQ8ERFPRcQNRzjmHRGxOiJWRcTtxW3nRcQDxW2/iIh3lrP4uuHkd0mS6k7LWAdERDNwE3AJ0AusiIjlKaXVJccsBW4ELkopbY+Ik4q79gDvSSn9MiI6gZ9GxL0ppR1lP5NatmAJEFlf1mnL8q5GkiSVwXhWsi4AnkoprU0pHQDuAC4fccw1wE0ppe0AKaXNxecnU0q/LL7eAGwGOspVfN2YNh3mdnqHoSRJdWQ8IWshsK7kfW9xW6nTgNMi4kcR8WBEXDryh0TEBUAbcFiSiIhrI2JlRKzs6+sbf/X1pNPmd0mS6km5Gt9bgKXA64ArgS9ExAnDOyOiAPwN8N6U0tDID6eUbk4pLUspLevoaNCFrkJ3NvV9l1dSJUmqB+MJWeuBRSXvu4rbSvUCy1NK/Smlp4EnyUIXETEb+DbwsZTSg5MvuU45+V2SpLoynpC1AlgaEUsiog24Alg+4phvkq1iERHzyC4fri0e/w3gyymlu8pWdT0qLMme7cuSJKkujBmyUkoDwHXAvcAa4M6U0qqI+GREXFY87F5ga0SsBr4PfDSltBV4B3AxcHVEPFx8nFeRM6l17cfBiQUnv0uSVCfGHOEAkFK6G7h7xLaPl7xOwPXFR+kxfwv87eTLbBCFHuh9Iu8qJElSGTjxvZp0dsMLfbBnZ96VSJKkSTJkVROb3yVJqhuGrGpS6M6e7cuSJKnmGbKqyfSZcMJ87zCUJKkOGLKqTWe3lwslSaoDhqxqU+iB7c/D3l15VyJJkibBkFVtbH6XJKkuGLKqzXDzu31ZkiTVNENWtTluNhzfARtcyZIkqZYZsqpRoduVLEmSapwhqxp19sC2jbBvd96VSJKkY2TIqkYv9WU9nW8dkiTpmBmyqtFLdxh6yVCSpFplyKpGM0+A2XMd4yBJUg0zZFUrm98lSapphqxqVeiBLRtg/968K5EkScfAkFWtCt1Agk02v0uSVIsMWdWq0+Z3SZJqmSGrWs06EWbOcfK7JEk1ypBVzWx+lySpZhmyqllnD2xZDwf25V2JJEmaIENWNSt0QxqCTc/kXYkkSZogQ1Y1e2nyu31ZkiTVGkNWNZs9F4473r4sSZJqkCGrmkVklww3GLIkSao1hqxqV+iBvnXQvz/vSiRJ0gQYsqrdcPP788/mXYkkSZoAQ1a167T5XZKkWmTIqnbHd8D0WfZlSZJUYwxZ1W64+d07DCVJqimGrFrQ2QObn4OB/rwrkSRJ42TIqgWFbhgatPldkqQaYsiqBU5+lySp5hiyasGc+dB+nH1ZkiTVEENWLXDyuyRJNceQVSsKPbD5WZvfJUmqEYasWtHZA4MD2VfsSJKkqmfIqhWF7uzZ5ndJkmqCIatWzFkA02bYlyVJUo0wZNWKpiZYsMQ7DCVJqhGGrFrS2QObnsl6syRJUlUzZNWSQg8M9kNfb96VSJKkMRiyaonN75Ik1QxDVi2Z2wlt7fZlSZJUA8YVsiLi0oh4IiKeiogbjnDMOyJidUSsiojbS7bfExE7IuJb5Sq6YQ03v3uHoSRJVW/MkBURzcBNwJuBs4ArI+KsEccsBW4ELkop/RrwkZLdnwbeXbaKG12hBzY9DUODeVciSZKOYjwrWRcAT6WU1qaUDgB3AJePOOYa4KaU0naAlNLm4R0ppe8BL5apXnX2wMAB2LI+70okSdJRjCdkLQRKv8ult7it1GnAaRHxo4h4MCIunUgREXFtRKyMiJV9fX0T+WjjGW5+95KhJElVrVyN7y3AUuB1wJXAFyLihPF+OKV0c0ppWUppWUdHR5lKqlPzFkJLm3cYSpJU5cYTstYDi0redxW3leoFlqeU+lNKTwNPkoUulVtTs5PfJUmqAeMJWSuApRGxJCLagCuA5SOO+SbZKhYRMY/s8qFLLZXS2QMbn4ahobwrkSRJRzBmyEopDQDXAfcCa4A7U0qrIuKTEXFZ8bB7ga0RsRr4PvDRlNJWgIj4IfA14A0R0RsRb6rEiTSUQg/074OtG/KuRJIkHUHLeA5KKd0N3D1i28dLXifg+uJj5GdfM8kaNdJLk99/BR1d+dYiSZJG5cT3WtSxyOZ3SZKqnCGrFjU3w/xTHOMgSVIVM2TVqpcmv9v8LklSNTJk1arOHti/B7ZvyrsSSZI0CkNWrXLyuyRJVc2QVas6FkFzi83vkiRVKUNWrWpphfmLnfwuSVKVMmTVskJ3tpKVUt6VSJKkEQxZtazQA/t2w/bn865EkiSNYMiqZaWT3yVJUlUxZNWy+adAk83vkiRVI0NWLWtphZNOdoyDJElVyJBV6wrd2eVCm98lSaoqhqxa19kDe3fBC315VyJJkkoYsmqdk98lSapKhqxaN/8UiCab3yVJqjKGrFrXOi1rfneMgyRJVcWQVQ8K3dnlQpvfJUmqGoaselDogT07YefWvCuRJElFhqx64OR3SZKqjiGrHixYYvO7JElVxpBVD9qmwbyFjnGQJKmKGLLqRaHHlSxJkqqIIatedHbDru3w4ra8K5EkSRiy6kehJ3v2kqEkSVXBkFUvFiwBwkuGkiRVCUNWvZg2HeZ1OsZBkqQqYciqJ4Ue2OBKliRJ1cCQVU8K3fDiVti1I+9KJElqeIasetJZbH73kqEkSbkzZNWTBUuyZ5vfJUnKnSGrnrQfBycWHOMgSVIVMGTVGye/S5JUFQxZ9aazG17og907865EkqSGZsiqN8OT313NkiQpV4aselPozp69w1CSpFwZsurN9JkwZ74hS5KknBmy6lGh28nvkiTlzJBVjwo9sON52Lsr70okSWpYhqx61GnzuyRJeTNk1aPh5neHkkqSlBtDVj2aMRuO73AlS5KkHBmy6lWh2zsMJUnKkSGrXnX2wLaNsG933pVIktSQxhWyIuLSiHgiIp6KiBuOcMw7ImJ1RKyKiNtLtl8VEb8sPq4qV+Eaw0uT35/Otw5JkhpUy1gHREQzcBNwCdALrIiI5Sml1SXHLAVuBC5KKW2PiJOK208E/gBYBiTgp8XPbi//qegQpZPfl5ydby2SJDWg8axkXQA8lVJam1I6ANwBXD7imGuAm4bDU0ppc3H7m4D7UkrbivvuAy4tT+k6qpknwOy5Nr9LkpST8YSshcC6kve9xW2lTgNOi4gfRcSDEXHpBD5LRFwbESsjYmVfX9/4q9fRFXoc4yBJUk7K1fjeAiwFXgdcCXwhIk4Y74dTSjenlJallJZ1dHSUqSRR6IatG2D/3rwrkSSp4YwnZK0HFpW87ypuK9ULLE8p9aeUngaeJAtd4/msKqWzB0iwyeZ3SZKm2nhC1gpgaUQsiYg24Apg+Yhjvkm2ikVEzCO7fLgWuBd4Y0TMiYg5wBuL2zQVnPwuSVJuxry7MKU0EBHXkYWjZuDWlNKqiPgksDKltJyDYWo1MAh8NKW0FSAi/itZUAP4ZEppWyVORKOYdSLMnGPzuyRJORgzZAGklO4G7h6x7eMlrxNwffEx8rO3ArdOrkwds84eJ79LkpQDJ77Xu0I3bFkPB/blXYkkSQ3FkFXvCj2QhmDTM3lXIklSQzFk1bvSye+SJGnKGLLq3ey5cNzxNr9LkjTFDFn1LsLJ75Ik5cCQ1QgK3dC3Dvr3512JJEkNw5DVCDqLze/PP5t3JZIkNQxDViNw8rskSVPOkNUIju+A6bNsfpckaQoZshpBhJPfJUmaYoasRlHohs3PQf+BvCuRJKkhGLIaRaEHhgazoCVJkirOkNUonPwuSdKUMmQ1ijnzoX2mze+SJE0RQ1ajiMhWsxzjIEnSlDBkNZJCN2x+Fgb6865EkqS6Z8hqJJ09MDiQfcWOJEmqKENWIyn0ZM9eMpQkqeIMWY1kznyYNsM7DCVJmgKGrEbS1JT1ZXmHoSRJFWfIajSFbtj0TNabJUmSKsaQ1WgKPTDYD329eVciSVJdM2Q1ms5i87t9WZIkVZQhq9GcWIC2du8wlCSpwgxZjaapCRbY/C5JUqUZshpRoRs2PQ2Dg3lXIklS3TJkNaLOHhg4AFvX512JJEl1y5DViJz8LklSxRmyGtG8Tmid5h2GkiRVkCGrETU1w4IlNr9LklRBhqxGVeiGjU/DkM3vkiRVgiGrURV6oH8fbN2YdyWSJNUlQ1ajcvK7JEkVZchqVPO6oKXNOwwlSaoQQ1ajam6G+YttfpckqUIMWY2sUPx6naGhvCuRJKnuGLIaWWcPHNgL2zflXYkkSXXHkNXInPwuSVLFGLIaWUcXNLd4h6EkSRVgyGpkLa02v0uSVCGGrEY33PyeUt6VSJJUVwxZja7QA/t22/wuSVKZGbIa3UuT371kKElSORmyGt1JJ0NTi3cYSpJUZuMKWRFxaUQ8ERFPRcQNo+y/OiL6IuLh4uN9Jfv+JCIeKz7eWc7iVQYtrVnQciVLkqSyahnrgIhoBm4CLgF6gRURsTyltHrEoV9NKV034rNvBV4BnAdMA34QEd9JKe0sS/Uqj85uWPNg1vwekXc1kiTVhfGsZF0APJVSWptSOgDcAVw+zp9/FnB/SmkgpbQb+AVw6bGVqoop9MDeXbCjL+9KJEmqG+MJWQuBdSXve4vbRnp7RPwiIu6KiEXFbY8Al0bEjIiYB/wmsGiUzypPhe7s2aGkkiSVTbka3/8BWJxSOge4D7gNIKX0XeBu4MfAV4AHgMGRH46IayNiZUSs7OtzNWXKzT8FosmQJUlSGY0nZK3n0NWnruK2l6SUtqaU9hff3gK8smTfH6aUzkspXQIE8OTIX5BSujmltCyltKyjo2Oi56DJap1m87skSWU2npC1AlgaEUsiog24AlheekBEFEreXgasKW5vjoi5xdfnAOcA3y1H4SqzQnc2xsHJ75IklcWYdxemlAYi4jrgXqAZuDWltCoiPgmsTCktBz4cEZcBA8A24Orix1uBH0Z2x9pO4F0ppYHyn4YmrdADD/8j7NwKx8/LuxpJkmremCELIKV0N1lvVem2j5e8vhG4cZTP7SO7w1DVrrOk+d2QJUnSpDnxXZn5S7Lmdye/S5JUFoYsZdqmwbyFNr9LklQmhiwd1NljyJIkqUwMWTqo0A27tsPObXlXIklSzTNk6aBCT/bsUFJJkibNkKWDFiwBwkuGkiSVgSFLB02bDvM6vcNQkqQyMGTpUAWb3yVJKgdDlg5V6IYXt8KuHXlXIklSTTNk6VCdNr9LklQOhiwdakHx63U2eMlQkqTJMGTpUO0z4MSCK1mSJE2SIUuHc/K7JEmTZsjS4Qrd8EIf7N6ZdyWSJNUsQ5YO99Lkd1ezJEk6VoYsHc6v15EkadIMWTrc9ONgznwnv0uSNAmGLI3Oye+SJE2KIUujK3TDjudhz4t5VyJJUk0yZGl0w5PfNz2dbx2SJNUoQ5ZGVxie/G5fliRJx8KQpdHNmA3Hd3iHoSRJx8iQpSNz8rskScfMkKUjK3TDto2wb3felUiSVHMMWTqyl4aS2vwuSdJEGbJ0ZMPN7/ZlSZI0YYYsHdnME2D2XO8wlCTpGBiydHROfpck6ZgYsnR0nT2wdQPs35t3JZIk1RRDlo6u0A0kV7MkSZogQ5aO7qU7DA1ZkiRNhCFLRzdrDsyc4x2GkiRNkCFLY3PyuyRJE2bI0tgKPbBlPRzYl3clkiTVDEOWxlbohjQEm5z8LknSeBmyNLZOm98lSZooQ5bGNutEOO54J79LkjQBhiyNLcLJ75IkTZAhS+PT2QN966B/f96VSJJUEwxZGp+Xmt+fybsSSZJqgiFL4+Pkd0mSJsSQpfE5fh5Mn+Xkd0mSxsmQpfGJcPK7JEkTYMjS+BV6YPNz0H8g70okSap6hiyNX6EbhgZh87N5VyJJUtUbV8iKiEsj4omIeCoibhhl/9UR0RcRDxcf7yvZ96mIWBURayLif0ZElPMENIWc/C5J0ri1jHVARDQDNwGXAL3AiohYnlJaPeLQr6aUrhvx2V8HLgLOKW76Z+C1wA8mWbfycMJJ0D7Tye+SJI3DeFayLgCeSimtTSkdAO4ALh/nz09AO9AGTANageePpVBVgYjskqF3GEqSNKbxhKyFwLqS973FbSO9PSJ+ERF3RcQigJTSA8D3gY3Fx70ppTUjPxgR10bEyohY2dfXN+GT0BTq7IHnn4OB/rwrkSSpqpWr8f0fgMUppXOA+4DbACLiVOBMoIssmL0+Il4z8sMppZtTSstSSss6OjrKVJIqotANQwPZXYaSJOmIxhOy1gOLSt53Fbe9JKW0NaU0/KV2twCvLL5+G/BgSmlXSmkX8B3g1ZMrWbly8rskSeMynpC1AlgaEUsiog24AlheekBEFEreXgYMXxJ8DnhtRLRERCtZ0/thlwtVQ05cANNm2JclSdIYxry7MKU0EBHXAfcCzcCtKaVVEfFJYGVKaTnw4Yi4DBgAtgFXFz9+F/B64FGyJvh7Ukr/UP7T0JQZbn73DkNJko5qzJAFkFK6G7h7xLaPl7y+EbhxlM8NAu+fZI2qNoUe+MndMDgAzeP6R0iSpIbjxHdNXKEbBvuhb93Yx0qS1KAMWZo4J79LkjQmQ5Ym7sQCtLXblyVJ0lEYsjRxTU2wwMnvkiQdjSFLx2bhqbBhLTz4LRgazLsaSZKqjiFLx+ait8GSl8E9X4Qv/HtY/8u8K5IkqaoYsnRsZp4A7/rP8K/+Hby4Hb7wH+DbN8O+3XlXJklSVXDIkY5dBPzaRdDzcvj+7fCT78CaB+FN74WzfyPbL0lSg3IlS5PXPgPe/D645lMwey78/Wfgb/4LbN2Qd2WSJOXGkKXy6eyB9/0xvOXarEfrf30EfvBV6D+Qd2WSJE05Q5bKq6kZLngzXPeXcNar4Qd3wF99BH71SN6VSZI0pQxZqoxZc+Dtvw/v/kT2/m8+AXd9JmuSlySpARiyVFk958IH/gxedwWseQD+8rqsQd7ZWpKkOmfIUuW1tsHr3pmFrYVL4e6b4ZYb/FoeSVJdM2Rp6sxbCO/+A3j79fDClmyI6XdugX178q5MkqSyc06WplYEvOw1cOor4B//Dh66G1b9GN78u3DWrztbS5JUN1zJUj6mHwdvvRau+ZOsSf5rfwp/919h28a8K5MkqSwMWcrXwqXZENM3vw+eezybrfVPX4OB/rwrkyRpUgxZyl9TM1z41my21unnZ1/R81e/D08/mndlkiQdM0OWqsfsE7MvnP7//jMMDcBtH4ev/xns2pF3ZZIkTZghS9Vn6Svg3/w5XPyv4LEfZbO1Vt4LQ0N5VyZJ0rgZslSdWqfB638HPvBZWLAEvvU5uPVG2PR03pVJkjQuhixVt44uuOqT8Lbfg22b4PP/Du65FfbvzbsySZKOyjlZqn4RcO7r4LRl8H/+Bh781sHZWme+ytlakqSq5EqWasf0mfDbH4Df/SOYMQvu/BTc/oew/fm8K5Mk6TCGLNWeRafDtX8Kb3ovPLMKbvow/PDvna0lSaoqhizVpuZmePVlcN1fZHcjfu9v4XPXZ6FLkqQqYMhSbTt+HrzzP8DvfAwGDsBf/yf45l/A7hfyrkyS1OBsfFd9OG0ZLH4Z3P81+PE34YmfwG+9B17+BmjyvyUkSVPPv31UP9qmwW+9C/71Z6DjZPiH/wVf+hg8/2zelUmSGpAhS/XnpJPhvf8NLv8QbN2Q9Wp99zY4sC/vyiRJDcTLhapPEfDy18PpxdlaP/4mrPpRNlvrjAvzrk6S1ABcyVJ9mzEbLvsg/P//HaZNhzv+GL7y32HH5rwrkyTVOUOWGsPJZ8L7/wdc8h5Y+4tsttaPvgGDA3lXJkmqU4YsNY7mFrjobfDBv4Duc+G+L8Pn/y08tybvyiRJdciQpcZzQgdceSNccQPs2wO3/kf43zfBnp15VyZJqiM2vqtxnXEhLDkH/ulOeGA5PP4QvPGq7Muom5rzrk6SVOMipZR3DYdYtmxZWrlyZd5lqNE8/yx863Ow7vGsWf70C+DMYghrbcu7OklSlYqIn6aUlo22z5UsCWD+KfDeP8xWs1b/OBv38PP/A23tcOor4MxXZd+R2H5c3pVKkmqEIUsa1tQEZ706ewz0w9OPZqHr8Z9kwaupBbpfll1mPP0CmDUn74olSVXMy4XSWIYGoffJLHCteQi2bwICFp2eBa4zLoS5hbyrlCTl4GiXCw1Z0kSkBJufgzUPZitcm9Zm2086OQtbZ74KFizJJs5LkuqeIUuqlO2bi5cUH4TnHoc0BMd3FAPXhdkQVO9UlKS6ZciSpsLuF+CJFVno+tUjMNhfvFPx/Cx0dZ/rnYqSVGcmfXdhRFwK/DnQDNySUvrjEfuvBj4NrC9u+suU0i0R8ZvAZ0sOPQO4IqX0zYmdglQDjjseXvFb2WP/XnjqZ8Wm+Qfg59+D1mQpM5wAAA26SURBVHZY+nI441Vw2iu9U1GS6tyYK1kR0Qw8CVwC9AIrgCtTSqtLjrkaWJZSuu4oP+dE4CmgK6W050jHuZKlujPQD888lvVxPbECdm3P7lRccnaxcf4CmHVi3lVKko7BZFeyLgCeSimtLf6wO4DLgdVH/dTh/iXwnaMFLKkutbTCqS/PHm99P6x/MrtL8fGH4Nufh2/fDF2nZT1cZ1wIczvzrliSVAbjCVkLgXUl73uBC0c57u0RcTHZqtfvp5TWjdh/BfCZ0X5BRFwLXAtw8sknj6MkqUY1NcGiM7LHJe+BvnXFwPVg9oXV930ZOhZldymecSEUur1TUZJq1HguF/5L4NKU0vuK798NXFh6aTAi5gK7Ukr7I+L9wDtTSq8v2V8AfgF0ppT6j/b7vFyohrVj88FZXM+tKblT8YKsj+vkM6HZOxUlqZpM9nLhemBRyfsuDja4A5BS2lry9hbgUyN+xjuAb4wVsKSGdsJJ8Krfzh67d8KTK7LAtfK78NC3Yfqsg3cq9pwLrdPyrliSdBTjCVkrgKURsYQsXF0B/E7pARFRSCltLL69DFgz4mdcCdw4yVqlxnHcbHj5G7LH/r3wq4ezxvk1D8LD/5jdqXjqy7M+rqXLYLp3KkpStRkzZKWUBiLiOuBeshEOt6aUVkXEJ4GVKaXlwIcj4jJgANgGXD38+YhYTLYS9k9lr15qBNOmH/qdis+sKg5AfQjWPJANO118dtbHdfoFMNs7FSWpGjiMVKpVQ0Ow/pfFsPUgbCsuJnedlvVwnXEBzFuYb42SVOec+C7Vu5Sgrze7S3HNQ7DxV9n2jkUHp813dGUDU71bUZLKxpAlNZodffDET7IVrmdXZ3cqArTPzFa35i2EeV3Zc0cXnDDfOxcl6RhM+mt1JNWYEzrgwrdmjz07YcNa2NILW9Znz0/9PGugH9bUAnMLh4av4edp0/M7D0mqYYYsqd7NmA2nnpc9Su3dDVuLoauvGMA2r8u+b3F45Qtg1txsteuQFbAumDXHS4+SdBSGLKlRTT8ua5LvOu3Q7QP9sP35Q1e+tqyHh78PB/YePK5t+sHQVRrC5izIvkpIkhqcIUvSoVpas9DU0XXo9pTgxe0l4asYwJ55FH7xg4PHRROcuODwy47zupznJamhGLIkjU9ENoNr9onQfc6h+/bvha0bDr30uKUXfvkzGBo4eNzMOSWXHRfCvEXZ8+y52fc6SlIdMWRJmrxp06GzJ3uUGhyEHc8fetlxy3p47Eewb9fB41qnwdyFh/d+nViA1rapPRdJKhNDlqTKaW6GuZ3Z4/TzD25PCXa/cOhlxy29sO5xePSHQHG0TDRl3+k4cuTEvIVZQ78kVTFDlqSpFwEzT8gei3/t0H0H9sO2DSWXHYsh7OlHYeDAweOmz4LjO7JLjcfPhdnzstez52XvZ811FUxSrgxZkqpL2zRYsCR7lBoaghf6Dl523LoBXtiSbVu3BvbuOvxnzZh9MHwZxCRNMUOWpNrQ1ARz5mePpa88fP+B/bBzC+zcevD5heHncQSx4+ceDGAGMUllYMiSVB/aph1smj+SA/uKIawYxF4oCWQ7+uDZNYc25A87UhA7viSQORtM0giGLEmNo619YkHshRErY9s3Hz2IvRS6DGKSDFmSdKgpC2IjQ5hBTKo3hixJmqjxBLH9e+HFbYeHsBe2FIPYati3+/DPTZsB7ccd/pg+s/h6eP/Mw/e1tft9klIVMWRJUiVMmw7TxhHESnvEdm6FPTuz8DX82P78wdel3x05mmgqhrCZIwLayNB2hP0tbYY0qYwMWZKUl2nTR/+eyCMZHIT9wwFsT/a8d9ehoWzfroP79u2Gvm0HX5fOGRtNc8sEA9qMkhW247LPS3qJ/0ZIUq1obs76uo512v1A/4hANlpIGxHYtm86+H5o8Og/v3VaMYTNOPxS5vSZxdfF5+mzsgA3fVZxFc1eNNUfQ5YkNYqW1oOT9icqJeg/UFwpGxnS9oxYSSu+3rU9m9w/vLo2/HVJo2lrLwlgpY9iCJs+a/R99qGpihmyJElji8hmkbVNy+6EnKihwWIY25UFs8MeLxZD24vZ+y3rD+4b7D/yz21qLglhI8LYUUPbTC9vquL8J0ySVHlNzTBjVvaYqP79h4axvSVhbGRo27UjWz3b+yLs33P0n9s2/fAQNtrlzJH7XD3TOBmyJEnVrXVa9pjoCtrQ4MFLmqWPUVfTXoTN6w6upg0OHPnnNrVkAWzajCyotbVnNzG0TYdp7aNsm370bU1Nk/vfR1XLkCVJqk9Nx3ijQEojVs9Kg9mLB7ft35uN1TiwL1tBO7Dx0G3j1Trt0OD1UlAreT2tGMgOOW6U8GZoqyqGLEmSSkUUA017NpH/WAwNZUHtwN5i8Np3aAAbfn2kbbu2w/4N2fYJh7b2I6yujQxto4S34VXD1rbsuaX4urnFS6THwJAlSVK5NTUVB9JOh2NoQzvMcGibSFArDXiloW3/XuifQGiDbNBta1s2sPaQEHaU9xM5tk4DnSFLkqRqVxraymFkaCsNagMHsn39Bw593b9/9PcH9sLuFw7fP9bw2yMZLdC1lISxQ96PM8AVesr3v90EGLIkSWo05Q5to0lp4oFtvIFutH1H84E/g/mnVO5cj8CQJUmSyi/iYI9Xpb0U6I4Q0ObMr3wNozBkSZKk2nZIoCtHE1x5eJ+nJElSBRiyJEmSKsCQJUmSVAGGLEmSpAowZEmSJFWAIUuSJKkCDFmSJEkVYMiSJEmqAEOWJElSBRiyJEmSKsCQJUmSVAGGLEmSpAowZEmSJFWAIUuSJKkCDFmSJEkVECmlvGs4RES8CDyRdx0VMA/YkncRZVaP5wT1eV6eU+2ox/Oqx3OC+jwvz2niTkkpdYy2o6WCv/RYPZFSWpZ3EeUWESvr7bzq8ZygPs/Lc6od9Xhe9XhOUJ/n5TmVl5cLJUmSKsCQJUmSVAHVGLJuzruACqnH86rHc4L6PC/PqXbU43nV4zlBfZ6X51RGVdf4LkmSVA+qcSVLkiSp5hmyJEmSKqCqQlZEXBoRT0TEUxFxQ971lENE3BoRmyPisbxrKZeIWBQR34+I1RGxKiJ+L++aJisi2iPiJxHxSPGc/kveNZVLRDRHxM8j4lt511IuEfFMRDwaEQ9HxMq86ymHiDghIu6KiMcjYk1EvDrvmiYrIk4v/hkNP3ZGxEfyrmuyIuL3i/8/8VhEfCUi2vOuabIi4veK57Oqlv+MRvs7NyJOjIj7IuKXxec5U1VP1YSsiGgGbgLeDJwFXBkRZ+VbVVn8NXBp3kWU2QDwb1NKZwGvAj5YB39W+4HXp5TOBc4DLo2IV+VcU7n8HrAm7yIq4DdTSufV0UyfPwfuSSmdAZxLHfyZpZSeKP4ZnQe8EtgDfCPnsiYlIhYCHwaWpZTOBpqBK/KtanIi4mzgGuACsn/2/kVEnJpvVcfsrzn879wbgO+llJYC3yu+nxJVE7LI/nCfSimtTSkdAO4ALs+5pklLKd0PbMu7jnJKKW1MKf2s+PpFsr8MFuZb1eSkzK7i29bio+bvComILuCtwC1516Iji4jjgYuBLwKklA6klHbkW1XZvQH4VUrp2bwLKYMWYHpEtAAzgA051zNZZwIPpZT2pJQGgH8C/t+cazomR/g793LgtuLr24D/Z6rqqaaQtRBYV/K+lxr/i7sRRMRi4OXAQ/lWMnnFy2oPA5uB+1JKNX9OwJ8B/x4YyruQMkvAdyPipxFxbd7FlMESoA/4UvHS7i0RcVzeRZXZFcBX8i5islJK64E/BZ4DNgIvpJS+m29Vk/YY8JqImBsRM4C3AItyrqmc5qeUNhZfbwLmT9UvrqaQpRoTETOBvwc+klLamXc9k5VSGixe1ugCLiguodesiPgXwOaU0k/zrqUCfiOl9Aqy9oIPRsTFeRc0SS3AK4C/Sim9HNjNFF7SqLSIaAMuA76Wdy2TVeznuZwsGHcCx0XEu/KtanJSSmuAPwG+C9wDPAwM5lpUhaRsbtWUXaWoppC1nkOTc1dxm6pQRLSSBay/Syl9Pe96yql4meb71H4v3UXAZRHxDNnl99dHxN/mW1J5FFcTSCltJuvxuSDfiiatF+gtWT29iyx01Ys3Az9LKT2fdyFl8FvA0ymlvpRSP/B14NdzrmnSUkpfTCm9MqV0MbAdeDLvmsro+YgoABSfN0/VL66mkLUCWBoRS4r/1XMFsDznmjSKiAiy3pE1KaXP5F1POURER0ScUHw9HbgEeDzfqiYnpXRjSqkrpbSY7N+nf0wp1fR/cQNExHERMWv4NfBGsssdNSultAlYFxGnFze9AVidY0nldiV1cKmw6DngVRExo/j/hW+gDm5SiIiTis8nk/Vj3Z5vRWW1HLiq+Poq4H9P1S9umapfNJaU0kBEXAfcS3a3xq0ppVU5lzVpEfEV4HXAvIjoBf4gpfTFfKuatIuAdwOPFnuYAP5jSunuHGuarAJwW/Eu1ybgzpRS3Yw8qDPzgW9kf7/RAtyeUron35LK4kPA3xX/I3Mt8N6c6ymLYhC+BHh/3rWUQ0rpoYi4C/gZ2Z3WP6c+vorm7yNiLtAPfLBWb7wY7e9c4I+BOyPid4FngXdMWT1+rY4kSVL5VdPlQkmSpLphyJIkSaoAQ5YkSVIFGLIkSZIqwJAlSZJUAYYsSZKkCjBkSZIkVcD/BbSVEudnAk/JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.lineplot(\n",
    "    x=[epoch + 1 for epoch in range(len(train_state['train_loss']))],\n",
    "    y=train_state['train_loss'],\n",
    "    color='coral', \n",
    "    label='loss',\n",
    ")\n",
    "\n",
    "plt.xticks([epoch for epoch in range(len(train_state['train_loss']) + 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "model = model.to(args.device)\n",
    "loss_func = torch.nn.MSELoss(reduction=\"sum\")\n",
    "dataset.set_split('test')\n",
    "\n",
    "batch_generator = generate_batches(\n",
    "    dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    device=args.device,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.encoder_only = True\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        x = batch_dict['x_data']\n",
    "        y_target = batch_dict['y_target']\n",
    "        \n",
    "        y_pred = model(x, None)\n",
    "        for pred, target in zip(y_pred, y_target):\n",
    "            predictions.append(pred.cpu().numpy().argmax())\n",
    "            targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n",
      "1490\n",
      "Counter({0: 717, 3: 208, 4: 188, 8: 176, 13: 67, 5: 29, 10: 29, 2: 27, 9: 24, 7: 12, 6: 7, 12: 3, 11: 3})\n",
      "Counter({'food': 887, 'staff': 352, 'ambience': 251})\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "print(len(targets))\n",
    "print(Counter(predictions))\n",
    "print(Counter(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chicken potato onion tomato sauce mushroom beef fried grilled bean\n",
      "1 recommend tuna favorite best gras highly grilled seared roasted shrimp\n",
      "2 pay money leave didn dont unless wouldn want order would\n",
      "3 table u minute asked manager min seated waiter seat tip\n",
      "4 wall room space ceiling lit lighting floor window wood booth\n",
      "5 dish menu entree course flavor item plate ingredient prix portion\n",
      "6 birthday went saturday reservation friend friday week night anniversary review\n",
      "7 review year lived worst eaten ny experience life nyc ve\n",
      "8 service staff waitstaff attentive friendly server courteous helpful rude professional\n",
      "9 month time minute week hour year sit ago reservation waiting\n",
      "10 great drink wine sangria nice perfect excellent good romantic amazing\n",
      "11 gras crust rib pork foie tender creme filet priced tuna\n",
      "12 cream wall chocolate butter fruit sauce sweet ice tomato lemon\n",
      "13 joint chinese neighborhood japanese american cuisine fare mexican indian standard\n"
     ]
    }
   ],
   "source": [
    "for i, aspect in enumerate(model.get_aspect_words(w2v)):\n",
    "    print(i, \" \".join([a for a in aspect]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     ambience       0.83      0.62      0.71       251\n",
      "    anecdotes       0.00      0.00      0.00         0\n",
      "         food       0.89      0.85      0.87       887\n",
      "miscellaneous       0.00      0.00      0.00         0\n",
      "        price       0.00      0.00      0.00         0\n",
      "        staff       0.68      0.75      0.71       352\n",
      "\n",
      "     accuracy                           0.79      1490\n",
      "    macro avg       0.40      0.37      0.38      1490\n",
      " weighted avg       0.83      0.79      0.80      1490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update cluster map manually after each training\n",
    "\n",
    "# possible labels\n",
    "LABELS = ['ambience', 'food', 'miscellaneous', 'price', 'staff', 'anecdotes']\n",
    "cluster_map = {\n",
    "    0: 'food', \n",
    "    1: 'food', \n",
    "    2: 'price', \n",
    "    3: 'staff',\n",
    "    4: 'ambience', \n",
    "    5: 'food', \n",
    "    6: 'anecdotes',  \n",
    "    7: 'miscellaneous', \n",
    "    8: 'staff', \n",
    "    9: 'miscellaneous', \n",
    "    10: 'food', \n",
    "    11: 'food', \n",
    "    12: 'food', \n",
    "    13: 'food'\n",
    "}\n",
    "\n",
    "y_pred = [cluster_map[pred] for pred in predictions]\n",
    "y_true = targets\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
