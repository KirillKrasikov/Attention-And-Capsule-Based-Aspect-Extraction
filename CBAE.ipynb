{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import gensim\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.num_lines = sum(1 for line in open(filename))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in tqdm(\n",
    "            codecs.open(self.filename, \"r\", encoding=\"utf-8\"), \n",
    "            self.filename, \n",
    "            self.num_lines\n",
    "        ):\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batches(path, batch_size=50, minlength=3):\n",
    "    \"\"\"\n",
    "        Reading batched texts of given min. length\n",
    "    :param path: path to the text file ``one line -- one normalized sentence''\n",
    "    :return: batches iterator\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip().split()\n",
    "\n",
    "        # lines with less than `minlength` words are omitted\n",
    "        if len(line) >= minlength:\n",
    "            batch.append(line)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(path, batch_size=50, minlength=5):\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "\n",
    "        if len(line) >= minlength:\n",
    "            batch_count += 1\n",
    "            if batch_count >= batch_size:\n",
    "                count += 1\n",
    "                batch_count = 0\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vectors(text, w2v_model, maxlen, vocabulary):\n",
    "    \"\"\"\n",
    "        Token sequence -- to a list of word vectors;\n",
    "        if token not in vocabulary, it is skipped; the rest of\n",
    "        the slots up to `maxlen` are replaced with zeroes\n",
    "    :param text: list of tokens\n",
    "    :param w2v_model: gensim w2v model\n",
    "    :param maxlen: max. length of the sentence; the rest is just cut away\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    acc_vecs = []\n",
    "\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv and (vocabulary is None or word in vocabulary):\n",
    "            acc_vecs.append(w2v_model.wv[word])\n",
    "\n",
    "    # padding for consistent length with ZERO vectors\n",
    "    if len(acc_vecs) < maxlen:\n",
    "        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n",
    "\n",
    "    return acc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_tensors(\n",
    "    path, \n",
    "    batch_size=50, \n",
    "    vocabulary=None,\n",
    "    maxlen=100, \n",
    "    pad_value=0, \n",
    "    minsentlength=5,\n",
    "    w2v_model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "        Data for training the NN -- from text file to word vectors sequences batches\n",
    "    :param path:\n",
    "    :param batch_size:\n",
    "    :param vocabulary:\n",
    "    :param maxlen:\n",
    "    :param pad_value:\n",
    "    :param minsentlength:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for batch in read_data_batches(path, batch_size, minsentlength):\n",
    "        batch_vecs = []\n",
    "        batch_texts = []\n",
    "\n",
    "        for text in batch:\n",
    "            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n",
    "            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        yield np.stack(batch_vecs, axis=0), batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"\n",
    "        Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for ABAE aspects matrix initialization\n",
    "    \"\"\"\n",
    "\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output(input_, kernel, padding, stride):\n",
    "    \"\"\"Calculate the Output size in Convolution layer\n",
    "    \n",
    "    \"\"\"\n",
    "    return math.floor(((input_ - kernel + 2 * padding) / stride) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules, in_channels, out_channels, kernel_size, stride, conv_out_size):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=0\n",
    "            ) \n",
    "            for _ in range(num_capsules)\n",
    "        ])\n",
    "        \n",
    "        self._out_channels = out_channels\n",
    "        self._conv_out_size = conv_out_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), self._out_channels * self._conv_out_size , -1)\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules, num_routes, in_channels, out_channels):\n",
    "        super(SecondaryCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1)).to(args.device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_in_ch,\n",
    "        conv_out_ch,\n",
    "        conv_kernel,\n",
    "        conv_stride,\n",
    "        prime_num_capsules,\n",
    "        prime_out_ch,\n",
    "        prime_kernel,\n",
    "        prime_stride,\n",
    "        secondary_num_capsules,\n",
    "        secondary_out_channels,\n",
    "        batch_size,\n",
    "        input_len,\n",
    "    ):\n",
    "        super(CapsNet, self).__init__()\n",
    "        \n",
    "        self.conv_layer = ConvLayer(\n",
    "            in_channels=conv_in_ch,\n",
    "            out_channels=conv_out_ch,\n",
    "            kernel_size=conv_kernel,\n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        conv_layer_output = calculate_conv_output(\n",
    "            input_=input_len, \n",
    "            kernel=conv_kernel, \n",
    "            padding=0, \n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        \n",
    "        prime_caps_conv_output = calculate_conv_output(\n",
    "            input_=conv_layer_output, \n",
    "            kernel=prime_kernel, \n",
    "            padding=0, \n",
    "            stride=prime_stride,\n",
    "        )\n",
    "        \n",
    "        self.primary_caps = PrimaryCaps(\n",
    "            num_capsules=prime_num_capsules, \n",
    "            in_channels=conv_out_ch, \n",
    "            out_channels=prime_out_ch, \n",
    "            kernel_size=prime_kernel, \n",
    "            stride=prime_stride,\n",
    "            conv_out_size=prime_caps_conv_output,\n",
    "        )\n",
    "        \n",
    "        self.secondary_caps = SecondaryCaps(\n",
    "            num_capsules=secondary_num_capsules,\n",
    "            num_routes=prime_caps_conv_output * prime_out_ch,\n",
    "            in_channels=prime_num_capsules,\n",
    "            out_channels=secondary_out_channels,\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(secondary_out_channels * secondary_num_capsules, input_len)\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._secondary_out_size=secondary_out_channels * secondary_num_capsules\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.secondary_caps(self.primary_caps(self.conv_layer(data)))\n",
    "        output = output.reshape(self._batch_size, self._secondary_out_size)\n",
    "\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim, \n",
    "        asp_count,\n",
    "        ortho_reg, \n",
    "        maxlen, \n",
    "        init_aspects_matrix,\n",
    "        cn_conv_out_ch,\n",
    "        cn_conv_kernel,\n",
    "        cn_conv_stride,\n",
    "        cn_prime_num_capsules,\n",
    "        cn_prime_out_ch,\n",
    "        cn_prime_kernel,\n",
    "        cn_prime_stride,\n",
    "        cn_secondary_num_capsules,\n",
    "        cn_secondary_out_channels,\n",
    "        batch_size,\n",
    "        encoder_only=False,\n",
    "    ):\n",
    "        super(CBAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.ortho = ortho_reg\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.caps_net = CapsNet(\n",
    "            conv_in_ch=wv_dim,\n",
    "            conv_out_ch=cn_conv_out_ch,\n",
    "            conv_kernel=cn_conv_kernel,\n",
    "            conv_stride=cn_conv_stride,\n",
    "            prime_num_capsules=cn_prime_num_capsules,\n",
    "            prime_out_ch=cn_prime_out_ch,\n",
    "            prime_kernel=cn_prime_kernel,\n",
    "            prime_stride=cn_prime_stride,\n",
    "            secondary_num_capsules=cn_secondary_num_capsules,\n",
    "            secondary_out_channels=cn_secondary_out_channels,\n",
    "            batch_size=batch_size,\n",
    "            input_len=maxlen,\n",
    "        )\n",
    "        \n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax()\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "            \n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Takes embeddings of a sentence as input, returns attention weights\n",
    "\n",
    "        \"\"\"\n",
    "        # compute attention scores, looking at text embeddings average\n",
    "        caps_weights = self.caps_net(text_embeddings.permute(0, 2, 1))\n",
    "\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(caps_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return caps_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "        \n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "\n",
    "        if self.encoder_only:\n",
    "            return aspects_importances\n",
    "        else:\n",
    "            # negative samples are averaged\n",
    "            averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
    "\n",
    "            # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "            recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "\n",
    "            # loss\n",
    "            reconstruction_triplet_loss = CBAE._reconstruction_loss(\n",
    "                weighted_text_emb,\n",
    "                recovered_emb,\n",
    "                averaged_negative_samples,\n",
    "            )\n",
    "\n",
    "            max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
    "            reconstruction_triplet_loss\n",
    "\n",
    "            return self.ortho * self._ortho_regularizer() + max_margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
    "\n",
    "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
    "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
    "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
    "\n",
    "        return reconstruction_triplet_loss\n",
    "\n",
    "    def _ortho_regularizer(self):\n",
    "        return torch.norm(\n",
    "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
    "            - torch.eye(self.asp_count).to(args.device))\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=15):\n",
    "        words = []\n",
    "\n",
    "        # getting aspects embeddings\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "\n",
    "        # getting scalar products of word embeddings and aspect embeddings;\n",
    "        # to obtain the ``probabilities'', one should also apply softmax\n",
    "        words_scores = w2v_model.wv.vectors.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data='restaurant/train.txt',\n",
    "    test_data='restaurant/test.txt',\n",
    "    test_labels='restaurant/test_label.txt',\n",
    "\n",
    "    w2v_file='embeddings.w2v',\n",
    "    w2v_size=200,\n",
    "    w2v_window=10,\n",
    "    w2v_min_count=5,\n",
    "    w2v_workers=7,\n",
    "    w2v_sg=1,\n",
    "    w2v_negative=5,\n",
    "    w2v_iter=1,\n",
    "    w2v_max_vocab_size=20000,\n",
    "\n",
    "    batch_size=50,\n",
    "    aspects_number=14,\n",
    "    ortho_reg=0.1,\n",
    "    epochs=1,\n",
    "    optimizer='adam',\n",
    "    neg_samples=5,\n",
    "    maxlen=32,\n",
    "\n",
    "    cn_conv_out_channels = 256,\n",
    "    cn_conv_kernel = 9,\n",
    "    cn_conv_stride = 1,\n",
    "    cn_prime_num_capsules=8,\n",
    "    cn_prime_kernel=3,\n",
    "    cn_prime_out_channels=32,\n",
    "    cn_prime_stride=2,\n",
    "    cn_secondary_num_capsules=10,\n",
    "    cn_secondary_out_channels=16,\n",
    "    \n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a788bd078fe4455b5f8df31c42a1c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='restaurant/train.txt', max=279885.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24b6a4afe164847a1aa11d82d0418fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='restaurant/train.txt', max=279885.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings.w2v saved\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading vectorizer\")\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    sentences = Sentences(args.data)\n",
    "    w2v = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        size=args.w2v_size, \n",
    "        window=args.w2v_window, \n",
    "        min_count=args.w2v_min_count, \n",
    "        workers=args.w2v_workers, \n",
    "        sg=args.w2v_sg,\n",
    "        negative=args.w2v_negative, \n",
    "        iter=args.w2v_iter, \n",
    "        max_vocab_size=args.w2v_max_vocab_size,\n",
    "    )\n",
    "    w2v.save(args.w2v_file)\n",
    "    print(f'{args.w2v_file} saved')\n",
    "    \n",
    "vectorizer = gensim.models.Word2Vec.load(args.w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he not in vocab\n",
      "love ['awesome', 'loved', 'hooked', 'lover', 'gotta', 'addicted', 'fabulous', 'crave', 'winner', 'fantastic']\n",
      "looks not in vocab\n",
      "buy ['sell', 'diet', 'buying', 'case', 'fortune', 'warning', 'pocket', 'rip', 'ton', 'expanding']\n",
      "laptop ['clipboard', 'usher', 'jacket', 'reservationist', 'shoved', 'courteously', 'midnite', 'greated', 'aisle', 'cc']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"he\", \"love\", \"looks\", \"buy\", \"laptop\"]:\n",
    "    if word in vectorizer.wv.vocab:\n",
    "        print(word, [w for w, c in vectorizer.wv.similar_by_word(word=word)])\n",
    "    else:\n",
    "        print(word, \"not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = vectorizer.vector_size\n",
    "y = torch.zeros(args.batch_size).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBAE(\n",
       "  (caps_net): CapsNet(\n",
       "    (conv_layer): ConvLayer(\n",
       "      (conv): Conv1d(200, 256, kernel_size=(9,), stride=(1,))\n",
       "    )\n",
       "    (primary_caps): PrimaryCaps(\n",
       "      (capsules): ModuleList(\n",
       "        (0): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (1): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (2): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (3): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (4): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (5): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (6): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (7): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "    (secondary_caps): SecondaryCaps()\n",
       "    (fc): Linear(in_features=160, out_features=32, bias=True)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=200, out_features=14, bias=True)\n",
       "  (softmax_aspects): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBAE(\n",
    "    wv_dim=wv_dim,\n",
    "    asp_count=args.aspects_number,\n",
    "    ortho_reg=args.ortho_reg, \n",
    "    maxlen=args.maxlen, \n",
    "    init_aspects_matrix=get_centroids(vectorizer, aspects_count=args.aspects_number),\n",
    "    cn_conv_out_ch=args.cn_conv_out_channels,\n",
    "    cn_conv_kernel=args.cn_conv_kernel,\n",
    "    cn_conv_stride=args.cn_conv_stride,\n",
    "    cn_prime_num_capsules=args.cn_prime_num_capsules,\n",
    "    cn_prime_out_ch=args.cn_prime_out_channels,\n",
    "    cn_prime_kernel=args.cn_prime_kernel,\n",
    "    cn_prime_stride=args.cn_prime_stride,\n",
    "    cn_secondary_num_capsules=args.cn_secondary_num_capsules,\n",
    "    cn_secondary_out_channels=args.cn_secondary_out_channels,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5005529b5b4d3195cd05efbd1e2125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=1.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a124e117eea4a5eb4559919232415df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=5564.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "0 batches, and LR: 0.001\n",
      "1 min minute bill tip 45 waited refill apology card asking ignored charged refused asked tax\n",
      "2 spumoni min minute sauteed souk goat frisco advance 45 mashed marinated seared tomato reservation waited\n",
      "3 intimate laid outdoor cozy comfortable lit relaxing hip relaxed quiet cramped ceiling inviting vibe romantic\n",
      "4 reasonably extensive reasonable knowledgeable creative priced portion varied selection value affordable prompt tasty knowledgable generous\n",
      "5 min souk advance reservation pm minute celebrate spumoni 45 ago seated saturday waited 30pm valentine\n",
      "6 sauteed marinated mashed goat spinach seared potato asparagus tomato roasted avocado tangy onion bass squid\n",
      "7 min seated minute hostess promptly reservation manager greeted waited rushed waitress ignored waiter empty apology\n",
      "8 ago celebrate advance week valentine month saturday anniversary dined last pm friday reservation tuesday monday\n",
      "9 chocolate banana cream ice pudding brulee cake vanilla coconut tart goat mango desert sorbet bread\n",
      "10 min minute 45 waited advance reservation pm rushed promptly ignored seated 30 waiting 30pm empty\n",
      "11 souk lived upper slope visiting west gem village east stumbled york hidden district living neighborhood\n",
      "12 attentive helpful knowledgeable courteous prompt efficient knowledgable friendly professional polite staff laid welcoming accomodating gracious\n",
      "13 reasonably spumoni waste reasonable penny souk somewhere value money fusion eaten upper anywhere lived authentic\n",
      "14 sauteed marinated goat mashed seared tomato asparagus tangy potato spinach avocado roasted squid creamy onion\n",
      "Loss: 3747.137451171875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-bf1df5f746d4>:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aspects_importances = self.softmax_aspects(raw_importances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 batches, and LR: 0.001\n",
      "1 bill tip min card credit refill minute charged tax charge 20 apology gratuity asking per\n",
      "2 sauteed jolt cream creamy roasted mashed coconut avocado marinated almond java vanilla egg tangy pork\n",
      "3 ceiling lit outdoor lighting intimate cozy wood exposed chair dimly fireplace downstairs hip banquette brick\n",
      "4 reasonably extensive selection priced reasonable varied creative affordable tasty variety inventive interesting decent menu range\n",
      "5 celebrate reservation birthday saturday stumbled night recommend highly visiting advance friday anniversary pm valentine thursday\n",
      "6 tomato marinated sauce onion potato spinach sauteed mashed bass mushroom seared lamb grilled roasted braised\n",
      "7 seated hostess minute min waitress manager waiter table reservation greeted promptly server host u seat\n",
      "8 ago week dined month celebrate valentine last saturday anniversary past friday visited monday went year\n",
      "9 chocolate ice cream banana cake pudding coffee dessert vanilla tea tart desert brulee creme chip\n",
      "10 min minute 45 advance waited hour ignored told pm asked didn reservation waiting empty apology\n",
      "11 slope lived west upper village ave york gem stumbled east hidden district park visiting avenue\n",
      "12 attentive knowledgeable helpful friendly professional courteous efficient knowledgable polite prompt staff welcoming service accomodating gracious\n",
      "13 anywhere lived fusion authentic waste chinese upper indian eaten east compared thai worst joint write\n",
      "14 marinated sauteed seared tangy braised asparagus caper squash creamy yellow roasted shredded atop fennel crusted\n",
      "Loss: 0.02107483334839344\n",
      "\n",
      "2000 batches, and LR: 0.001\n",
      "1 bill tip min card credit refill charged minute tax charge apology gratuity 20 asking per\n",
      "2 sauteed jolt cream creamy roasted coconut mashed java almond avocado vanilla marinated tangy egg pork\n",
      "3 ceiling lit outdoor lighting intimate wood cozy exposed chair dimly fireplace banquette brick downstairs modern\n",
      "4 reasonably extensive selection creative varied priced reasonable affordable tasty variety inventive interesting menu decent range\n",
      "5 celebrate reservation birthday saturday stumbled night highly recommend visiting advance friday anniversary recommended valentine thursday\n",
      "6 tomato marinated sauce onion potato spinach sauteed mashed bass mushroom seared lamb grilled roasted braised\n",
      "7 seated hostess minute min waitress manager waiter table reservation greeted promptly server host u seat\n",
      "8 ago week dined month celebrate valentine last saturday anniversary past visited friday monday went year\n",
      "9 chocolate ice cream banana cake pudding coffee dessert vanilla tea tart desert brulee creme chip\n",
      "10 min minute 45 advance waited ignored hour told asked didn pm empty waiting reservation apology\n",
      "11 slope west lived upper village ave york gem stumbled hidden east district park visiting avenue\n",
      "12 attentive knowledgeable helpful friendly professional courteous efficient knowledgable polite prompt staff welcoming service accomodating gracious\n",
      "13 anywhere lived fusion authentic chinese waste upper indian eaten east thai compared worst joint write\n",
      "14 marinated sauteed seared tangy braised asparagus caper squash yellow creamy roasted atop shredded fennel drizzled\n",
      "Loss: 0.0003346351149957627\n",
      "\n",
      "3000 batches, and LR: 0.001\n",
      "1 bill tip min card credit refill charged tax 20 charge minute gratuity apology per asking\n",
      "2 min reservation jolt dry mashed sauteed cream egg boomer pork java vanilla roasted coconut review\n",
      "3 chair lit outdoor intimate ceiling lighting downstairs private cozy cramped loud comfortable dimly upstairs quiet\n",
      "4 reasonably extensive selection creative varied priced reasonable tasty variety affordable inventive interesting menu decent range\n",
      "5 celebrate birthday highly recommend stumbled night saturday reservation visiting recommended visited anniversary went friday valentine\n",
      "6 tomato marinated sauce onion spinach potato sauteed mashed bass seared lamb mushroom grilled roasted braised\n",
      "7 seated hostess minute min waitress manager waiter reservation table promptly greeted server host u apology\n",
      "8 ago week dined month valentine celebrate last saturday anniversary past visited friday monday year went\n",
      "9 chocolate ice cream banana cake pudding coffee dessert vanilla tart tea brulee desert creme chip\n",
      "10 min 45 minute waited advance hour ignored told asked didn worst pm bad arrived apology\n",
      "11 slope lived west upper village ave york gem stumbled east district hidden park visiting avenue\n",
      "12 attentive knowledgeable helpful friendly professional courteous efficient knowledgable polite prompt staff welcoming service accomodating gracious\n",
      "13 fusion anywhere authentic lived indian upper chinese thai east compared waste eaten joint west mexican\n",
      "14 marinated sauteed seared asparagus braised tangy roasted caper squash sliced fennel atop drizzled crusted tomato\n",
      "Loss: 6.589134864043444e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_bar = tqdm(\n",
    "    desc='training routine', \n",
    "    total=args.epochs,\n",
    "    position=1,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "train_bar = tqdm(\n",
    "    desc='train',\n",
    "    total=get_num_batches(args.data, args.batch_size), \n",
    "    position=1, \n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "for t in range(args.epochs):\n",
    "\n",
    "    print(f'Epoch {t + 1}/{args.epochs}')\n",
    "\n",
    "    data_iterator = read_data_tensors(\n",
    "        args.data,\n",
    "        batch_size=args.batch_size, \n",
    "        maxlen=args.maxlen,\n",
    "        w2v_model=vectorizer,\n",
    "    )\n",
    "\n",
    "    for item_number, (x, texts) in enumerate(data_iterator):\n",
    "        if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
    "            x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
    "\n",
    "        x = torch.from_numpy(x).to(args.device)\n",
    "\n",
    "        # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
    "        negative_samples = torch.stack(\n",
    "            tuple([x[torch.randperm(x.shape[0])[:args.neg_samples]] \n",
    "                   for _ in range(args.batch_size)])\n",
    "        ).to(args.device)\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model(x, negative_samples)\n",
    "\n",
    "        # error computation\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if item_number % 1000 == 0:\n",
    "\n",
    "            print(item_number, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            for i, aspect in enumerate(model.get_aspect_words(vectorizer)):\n",
    "                print(i + 1, \" \".join([a for a in aspect]))\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print()\n",
    "\n",
    "        train_bar.update()\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-bf1df5f746d4>:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aspects_importances = self.softmax_aspects(raw_importances)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.encoder_only = True\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    data_iterator = read_data_tensors(\n",
    "        args.test_data,\n",
    "        batch_size=args.batch_size, \n",
    "        maxlen=args.maxlen,\n",
    "        w2v_model=vectorizer,\n",
    "    )\n",
    "\n",
    "    for item_number, (x, texts) in enumerate(data_iterator):\n",
    "        if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
    "            x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
    "\n",
    "        x = torch.from_numpy(x).to(args.device)\n",
    "\n",
    "        y_pred = model(x, None)\n",
    "        \n",
    "        for pred in y_pred:\n",
    "            predictions.append(pred.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for pred in predictions:\n",
    "    classes.append(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({13: 1115, 4: 11, 1: 6, 11: 18})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
