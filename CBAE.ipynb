{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import gensim\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.num_lines = sum(1 for line in open(filename))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in tqdm(\n",
    "            codecs.open(self.filename, \"r\", encoding=\"utf-8\"), \n",
    "            self.filename, \n",
    "            self.num_lines\n",
    "        ):\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batches(path, batch_size=50, minlength=3):\n",
    "    \"\"\"\n",
    "        Reading batched texts of given min. length\n",
    "    :param path: path to the text file ``one line -- one normalized sentence''\n",
    "    :return: batches iterator\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip().split()\n",
    "\n",
    "        # lines with less than `minlength` words are omitted\n",
    "        if len(line) >= minlength:\n",
    "            batch.append(line)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(path, batch_size=50, minlength=5):\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "\n",
    "        if len(line) >= minlength:\n",
    "            batch_count += 1\n",
    "            if batch_count >= batch_size:\n",
    "                count += 1\n",
    "                batch_count = 0\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vectors(text, w2v_model, maxlen, vocabulary):\n",
    "    \"\"\"\n",
    "        Token sequence -- to a list of word vectors;\n",
    "        if token not in vocabulary, it is skipped; the rest of\n",
    "        the slots up to `maxlen` are replaced with zeroes\n",
    "    :param text: list of tokens\n",
    "    :param w2v_model: gensim w2v model\n",
    "    :param maxlen: max. length of the sentence; the rest is just cut away\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    acc_vecs = []\n",
    "\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv and (vocabulary is None or word in vocabulary):\n",
    "            acc_vecs.append(w2v_model.wv[word])\n",
    "\n",
    "    # padding for consistent length with ZERO vectors\n",
    "    if len(acc_vecs) < maxlen:\n",
    "        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n",
    "\n",
    "    return acc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_tensors(\n",
    "    path, \n",
    "    batch_size=50, \n",
    "    vocabulary=None,\n",
    "    maxlen=100, \n",
    "    pad_value=0, \n",
    "    minsentlength=5,\n",
    "    w2v_model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "        Data for training the NN -- from text file to word vectors sequences batches\n",
    "    :param path:\n",
    "    :param batch_size:\n",
    "    :param vocabulary:\n",
    "    :param maxlen:\n",
    "    :param pad_value:\n",
    "    :param minsentlength:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for batch in read_data_batches(path, batch_size, minsentlength):\n",
    "        batch_vecs = []\n",
    "        batch_texts = []\n",
    "\n",
    "        for text in batch:\n",
    "            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n",
    "            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        yield np.stack(batch_vecs, axis=0), batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"\n",
    "        Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for ABAE aspects matrix initialization\n",
    "    \"\"\"\n",
    "\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output(input_, kernel, padding, stride):\n",
    "    \"\"\"Calculate the Output size in Convolution layer\n",
    "    \n",
    "    \"\"\"\n",
    "    return math.floor(((input_ - kernel + 2 * padding) / stride) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules, in_channels, out_channels, kernel_size, stride, conv_out_size):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=0\n",
    "            ) \n",
    "            for _ in range(num_capsules)\n",
    "        ])\n",
    "        \n",
    "        self._out_channels = out_channels\n",
    "        self._conv_out_size = conv_out_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), self._out_channels * self._conv_out_size , -1)\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules, num_routes, in_channels, out_channels):\n",
    "        super(SecondaryCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1)).to(args.device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_in_ch,\n",
    "        conv_out_ch,\n",
    "        conv_kernel,\n",
    "        conv_stride,\n",
    "        prime_num_capsules,\n",
    "        prime_out_ch,\n",
    "        prime_kernel,\n",
    "        prime_stride,\n",
    "        secondary_num_capsules,\n",
    "        secondary_out_channels,\n",
    "        batch_size,\n",
    "        input_len,\n",
    "    ):\n",
    "        super(CapsNet, self).__init__()\n",
    "        \n",
    "        self.conv_layer = ConvLayer(\n",
    "            in_channels=conv_in_ch,\n",
    "            out_channels=conv_out_ch,\n",
    "            kernel_size=conv_kernel,\n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        conv_layer_output = calculate_conv_output(\n",
    "            input_=input_len, \n",
    "            kernel=conv_kernel, \n",
    "            padding=0, \n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        \n",
    "        prime_caps_conv_output = calculate_conv_output(\n",
    "            input_=conv_layer_output, \n",
    "            kernel=prime_kernel, \n",
    "            padding=0, \n",
    "            stride=prime_stride,\n",
    "        )\n",
    "        \n",
    "        self.primary_caps = PrimaryCaps(\n",
    "            num_capsules=prime_num_capsules, \n",
    "            in_channels=conv_out_ch, \n",
    "            out_channels=prime_out_ch, \n",
    "            kernel_size=prime_kernel, \n",
    "            stride=prime_stride,\n",
    "            conv_out_size=prime_caps_conv_output,\n",
    "        )\n",
    "        \n",
    "        self.secondary_caps = SecondaryCaps(\n",
    "            num_capsules=secondary_num_capsules,\n",
    "            num_routes=prime_caps_conv_output * prime_out_ch,\n",
    "            in_channels=prime_num_capsules,\n",
    "            out_channels=secondary_out_channels,\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(secondary_out_channels * secondary_num_capsules, input_len)\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._secondary_out_size=secondary_out_channels * secondary_num_capsules\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.secondary_caps(self.primary_caps(self.conv_layer(data)))\n",
    "        output = output.reshape(self._batch_size, self._secondary_out_size)\n",
    "\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim, \n",
    "        asp_count,\n",
    "        ortho_reg, \n",
    "        maxlen, \n",
    "        init_aspects_matrix,\n",
    "        cn_conv_out_ch,\n",
    "        cn_conv_kernel,\n",
    "        cn_conv_stride,\n",
    "        cn_prime_num_capsules,\n",
    "        cn_prime_out_ch,\n",
    "        cn_prime_kernel,\n",
    "        cn_prime_stride,\n",
    "        cn_secondary_num_capsules,\n",
    "        cn_secondary_out_channels,\n",
    "        batch_size,\n",
    "    ):\n",
    "        super(CBAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.ortho = ortho_reg\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.caps_net = CapsNet(\n",
    "            conv_in_ch=wv_dim,\n",
    "            conv_out_ch=cn_conv_out_ch,\n",
    "            conv_kernel=cn_conv_kernel,\n",
    "            conv_stride=cn_conv_stride,\n",
    "            prime_num_capsules=cn_prime_num_capsules,\n",
    "            prime_out_ch=cn_prime_out_ch,\n",
    "            prime_kernel=cn_prime_kernel,\n",
    "            prime_stride=cn_prime_stride,\n",
    "            secondary_num_capsules=cn_secondary_num_capsules,\n",
    "            secondary_out_channels=cn_secondary_out_channels,\n",
    "            batch_size=batch_size,\n",
    "            input_len=maxlen,\n",
    "        )\n",
    "        \n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax()\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "\n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Takes embeddings of a sentence as input, returns attention weights\n",
    "\n",
    "        \"\"\"\n",
    "        # compute attention scores, looking at text embeddings average\n",
    "        caps_weights = self.caps_net(text_embeddings.permute(0, 2, 1))\n",
    "\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(caps_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return caps_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "\n",
    "        # negative samples are averaged\n",
    "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
    "        \n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "\n",
    "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "\n",
    "        # loss\n",
    "        reconstruction_triplet_loss = CBAE._reconstruction_loss(\n",
    "            weighted_text_emb,\n",
    "            recovered_emb,\n",
    "            averaged_negative_samples,\n",
    "        )\n",
    "        \n",
    "        max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
    "        reconstruction_triplet_loss\n",
    "\n",
    "        return self.ortho * self._ortho_regularizer() + max_margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
    "\n",
    "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
    "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
    "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
    "\n",
    "        return reconstruction_triplet_loss\n",
    "\n",
    "    def _ortho_regularizer(self):\n",
    "        return torch.norm(\n",
    "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
    "            - torch.eye(self.asp_count).to(args.device))\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=15):\n",
    "        words = []\n",
    "\n",
    "        # getting aspects embeddings\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "\n",
    "        # getting scalar products of word embeddings and aspect embeddings;\n",
    "        # to obtain the ``probabilities'', one should also apply softmax\n",
    "        words_scores = w2v_model.wv.vectors.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data='restaurant/train.txt',\n",
    "    \n",
    "    w2v_file='embeddings.w2v',\n",
    "    w2v_size=200,\n",
    "    w2v_window=10,\n",
    "    w2v_min_count=5,\n",
    "    w2v_workers=7,\n",
    "    w2v_sg=1,\n",
    "    w2v_negative=5,\n",
    "    w2v_iter=1,\n",
    "    w2v_max_vocab_size=20000,\n",
    "\n",
    "    batch_size=50,\n",
    "    aspects_number=14,\n",
    "    ortho_reg=0.1,\n",
    "    epochs=1,\n",
    "    optimizer='adam',\n",
    "    neg_samples=5,\n",
    "    maxlen=32,\n",
    "\n",
    "    cn_conv_out_channels = 256,\n",
    "    cn_conv_kernel = 9,\n",
    "    cn_conv_stride = 1,\n",
    "    cn_prime_num_capsules=8,\n",
    "    cn_prime_kernel=3,\n",
    "    cn_prime_out_channels=32,\n",
    "    cn_prime_stride=2,\n",
    "    cn_secondary_num_capsules=10,\n",
    "    cn_secondary_out_channels=16,\n",
    "    \n",
    "    cuda=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38243c7855742e7b8bde66f83efd826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='restaurant/train.txt', max=279885.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee690d609f641d2b7ec790e0749eb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='restaurant/train.txt', max=279885.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings.w2v saved\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading vectorizer\")\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    sentences = Sentences(args.data)\n",
    "    w2v = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        size=args.w2v_size, \n",
    "        window=args.w2v_window, \n",
    "        min_count=args.w2v_min_count, \n",
    "        workers=args.w2v_workers, \n",
    "        sg=args.w2v_sg,\n",
    "        negative=args.w2v_negative, \n",
    "        iter=args.w2v_iter, \n",
    "        max_vocab_size=args.w2v_max_vocab_size,\n",
    "    )\n",
    "    w2v.save(args.w2v_file)\n",
    "    print(f'{args.w2v_file} saved')\n",
    "    \n",
    "vectorizer = gensim.models.Word2Vec.load(args.w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he not in vocab\n",
      "love ['loved', 'awesome', 'lover', 'crave', 'liked', 'rock', 'gotta', 'raving', 'hooked', 'cuban']\n",
      "looks not in vocab\n",
      "buy ['sell', 'diet', 'favor', 'head', 'case', 'buying', 'soda', 'charge', 'fortune', 'store']\n",
      "laptop ['courtyard', 'chit', 'wander', 'gossiping', 'necessity', 'socializing', 'sight', 'directed', 'balloon', 'happend']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"he\", \"love\", \"looks\", \"buy\", \"laptop\"]:\n",
    "    if word in vectorizer.wv.vocab:\n",
    "        print(word, [w for w, c in vectorizer.wv.similar_by_word(word=word)])\n",
    "    else:\n",
    "        print(word, \"not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = vectorizer.vector_size\n",
    "y = torch.zeros(args.batch_size).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBAE(\n",
       "  (caps_net): CapsNet(\n",
       "    (conv_layer): ConvLayer(\n",
       "      (conv): Conv1d(200, 256, kernel_size=(9,), stride=(1,))\n",
       "    )\n",
       "    (primary_caps): PrimaryCaps(\n",
       "      (capsules): ModuleList(\n",
       "        (0): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (1): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (2): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (3): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (4): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (5): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (6): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (7): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "    (secondary_caps): SecondaryCaps()\n",
       "    (fc): Linear(in_features=160, out_features=32, bias=True)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=200, out_features=14, bias=True)\n",
       "  (softmax_aspects): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBAE(\n",
    "    wv_dim=wv_dim,\n",
    "    asp_count=args.aspects_number,\n",
    "    ortho_reg=args.ortho_reg, \n",
    "    maxlen=args.maxlen, \n",
    "    init_aspects_matrix=get_centroids(vectorizer, aspects_count=args.aspects_number),\n",
    "    cn_conv_out_ch=args.cn_conv_out_channels,\n",
    "    cn_conv_kernel=args.cn_conv_kernel,\n",
    "    cn_conv_stride=args.cn_conv_stride,\n",
    "    cn_prime_num_capsules=args.cn_prime_num_capsules,\n",
    "    cn_prime_out_ch=args.cn_prime_out_channels,\n",
    "    cn_prime_kernel=args.cn_prime_kernel,\n",
    "    cn_prime_stride=args.cn_prime_stride,\n",
    "    cn_secondary_num_capsules=args.cn_secondary_num_capsules,\n",
    "    cn_secondary_out_channels=args.cn_secondary_out_channels,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dfda6fe9c049f48ecda0d3f44e2a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=1.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed79cbc695ad4aa9abdf54c42d1b5cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=5564.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "0 batches, and LR: 0.001\n",
      "1 every twice advance ago waste first next last reservation 45 min waited least month several\n",
      "2 min souk minute reservation 45 waited advance pm seated promptly spumoni prompt frisco fleur rushed\n",
      "3 min minute 45 waited pm reservation advance seated 30 promptly waiting ignored empty reserved 30pm\n",
      "4 reasonably extensive reasonable creative inventive variety varied affordable selection priced inexpensive knowledgeable fixe unique fare\n",
      "5 marinated sauteed seared goat tomato roasted asparagus mashed potato creamy tempura spinach squid tangy avocado\n",
      "6 goat sauteed marinated seared roasted asparagus mashed potato tomato spinach avocado onion coconut squid tempura\n",
      "7 helpful knowledgeable prompt attentive courteous knowledgable efficient rushed polite promptly professional accomodating friendly rude hostess\n",
      "8 min souk marinated minute sauteed 45 waited spumoni tomato seared goat reservation roasted asparagus mashed\n",
      "9 staff bartender helpful knowledgeable attentive hostess prompt courteous waitress waitstaff waiter knowledgable server service professional\n",
      "10 intimate crowded relaxed quiet cozy laid relaxing outdoor cramped noisy comfortable souk romantic loud vibe\n",
      "11 min minute 45 pm waited reservation 30 seated 15 advance 30pm hour promptly 20 arrived\n",
      "12 lit ceiling wood exposed lighting chair banquette cozy comfortable outdoor intimate wall dimly wooden inviting\n",
      "13 celebrate ago souk advance visiting anniversary valentine saturday pm thursday month reservation week friday birthday\n",
      "14 souk waste somewhere money penny advance reasonably lived min elsewhere reasonable anywhere reservation visiting eaten\n",
      "Loss: 3552.44580078125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-847f41da9e5b>:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aspects_importances = self.softmax_aspects(raw_importances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 batches, and LR: 0.001\n",
      "1 every first next twice several least waste last never month ago week second back advance\n",
      "2 review citysearch reading stumbled block sushi souk reservation card credit manager negative refused phone advance\n",
      "3 min 45 minute tip bill refused advance told ignored waited apology per pm credit card\n",
      "4 reasonably extensive variety creative reasonable inventive selection affordable varied priced ingredient fare portion range inexpensive\n",
      "5 medium seared marinated overcooked tender cooked sliced mashed sauteed mignon roasted grilled rib braised crusted\n",
      "6 goat coconut pear roasted sauteed asparagus spinach potato shrimp mashed seared calamari bass scallop marinated\n",
      "7 helpful knowledgeable attentive courteous rude knowledgable efficient polite prompt professional friendly gracious accomodating slow service\n",
      "8 creamy atop sauteed wood vanilla jolt tangy yellow caper span squash tomato java topped marinated\n",
      "9 staff bartender waiter waitress waitstaff server helpful hostess knowledgeable service manager attentive host courteous professional\n",
      "10 crowded noisy loud quiet cramped laid outdoor cozy upper intimate hip sit seating village relax\n",
      "11 minute 45 min 30 waited pm reservation 15 arrived hour 20 seated 10 40 30pm\n",
      "12 lit wood ceiling exposed banquette lighting wall wooden chair fireplace dimly brick candle window outdoor\n",
      "13 celebrate anniversary visiting stumbled ago birthday went valentine visited recently dined decided reading took citysearch\n",
      "14 anywhere better waste elsewhere money chinatown best anyone indian slope lived fusion somewhere chinese recommend\n",
      "Loss: 0.011827677488327026\n",
      "\n",
      "2000 batches, and LR: 0.001\n",
      "1 every first next twice several least waste last never month week second ago back three\n",
      "2 review citysearch reading stumbled sushi block souk reservation card credit negative manager refused phone walking\n",
      "3 min tip 45 minute refused bill advance told ignored waited apology per credit card pm\n",
      "4 reasonably extensive variety creative reasonable inventive selection affordable varied ingredient priced fare portion range inexpensive\n",
      "5 medium seared marinated tender overcooked sliced cooked mashed sauteed roasted mignon rib grilled braised crusted\n",
      "6 goat coconut pear roasted sauteed asparagus spinach potato shrimp mashed seared calamari bass scallop marinated\n",
      "7 helpful knowledgeable attentive courteous rude knowledgable efficient polite prompt professional friendly gracious accomodating slow service\n",
      "8 creamy atop wood sauteed vanilla jolt tangy yellow caper span squash tomato topped java marinated\n",
      "9 staff bartender waiter waitress waitstaff server helpful hostess knowledgeable service manager attentive host courteous professional\n",
      "10 crowded noisy loud quiet laid cramped outdoor cozy upper hip intimate sit seating village vibe\n",
      "11 minute 45 min 30 waited pm reservation 15 arrived hour 20 seated 10 40 30pm\n",
      "12 lit wood ceiling exposed banquette lighting wall wooden chair fireplace brick dimly candle window outdoor\n",
      "13 celebrate anniversary visiting stumbled ago birthday went visited valentine recently dined decided reading citysearch took\n",
      "14 anywhere better waste elsewhere chinatown money best indian anyone slope lived fusion chinese recommend somewhere\n",
      "Loss: 0.00022674992214888334\n",
      "\n",
      "3000 batches, and LR: 0.001\n",
      "1 every first next twice several least waste last never month second week ago back three\n",
      "2 review citysearch reading stumbled sushi block souk reservation card credit negative manager refused phone walking\n",
      "3 min tip 45 minute bill refused told advance ignored waited apology per credit card pm\n",
      "4 reasonably variety extensive creative reasonable inventive selection affordable varied ingredient priced fare portion range inexpensive\n",
      "5 medium seared marinated tender overcooked cooked sliced mashed sauteed roasted mignon rib grilled braised seasoned\n",
      "6 goat coconut pear roasted sauteed asparagus spinach potato shrimp mashed seared calamari bass scallop avocado\n",
      "7 helpful knowledgeable attentive courteous rude knowledgable efficient polite prompt professional friendly gracious accomodating slow service\n",
      "8 creamy atop wood sauteed vanilla jolt tangy yellow span caper squash topped tomato java marinated\n",
      "9 staff bartender waiter waitress waitstaff server helpful hostess knowledgeable service manager host attentive courteous professional\n",
      "10 crowded noisy loud quiet cramped laid outdoor cozy upper hip intimate sit seating village vibe\n",
      "11 minute 45 min 30 waited pm reservation 15 arrived hour 20 seated 10 40 30pm\n",
      "12 lit wood ceiling exposed banquette lighting wall wooden chair fireplace brick dimly candle window outdoor\n",
      "13 celebrate anniversary visiting stumbled ago birthday went visited valentine recently dined decided reading citysearch took\n",
      "14 anywhere better waste elsewhere chinatown money best indian anyone slope lived fusion chinese recommend somewhere\n",
      "Loss: 3.230927331232536e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_bar = tqdm(\n",
    "    desc='training routine', \n",
    "    total=args.epochs,\n",
    "    position=0,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "train_bar = tqdm(\n",
    "    desc='train',\n",
    "    total=get_num_batches(args.data, args.batch_size), \n",
    "    position=1, \n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "for t in range(args.epochs):\n",
    "\n",
    "    print(f'Epoch {t + 1}/{args.epochs}')\n",
    "\n",
    "    data_iterator = read_data_tensors(\n",
    "        args.data,\n",
    "        batch_size=args.batch_size, \n",
    "        maxlen=args.maxlen,\n",
    "        w2v_model=vectorizer,\n",
    "    )\n",
    "\n",
    "    for item_number, (x, texts) in enumerate(data_iterator):\n",
    "        if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
    "            x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
    "\n",
    "        x = torch.from_numpy(x).to(args.device)\n",
    "\n",
    "        # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
    "        negative_samples = torch.stack(\n",
    "            tuple([x[torch.randperm(x.shape[0])[:args.neg_samples]] \n",
    "                   for _ in range(args.batch_size)])\n",
    "        ).to(args.device)\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model(x, negative_samples)\n",
    "\n",
    "        # error computation\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if item_number % 1000 == 0:\n",
    "\n",
    "            print(item_number, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            for i, aspect in enumerate(model.get_aspect_words(vectorizer)):\n",
    "                print(i + 1, \" \".join([a for a in aspect]))\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print()\n",
    "\n",
    "        train_bar.update()\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
