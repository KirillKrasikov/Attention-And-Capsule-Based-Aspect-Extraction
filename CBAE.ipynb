{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkrasikov/PycharmProjects/TopicModelingWithCapsNet/venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "from sklearn.cluster.k_means_ import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.num_lines = sum(1 for line in open(filename))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in tqdm(\n",
    "            codecs.open(self.filename, \"r\", encoding=\"utf-8\"), \n",
    "            self.filename, \n",
    "            self.num_lines\n",
    "        ):\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batches(path, batch_size=50, minlength=5):\n",
    "    \"\"\"\n",
    "        Reading batched texts of given min. length\n",
    "    :param path: path to the text file ``one line -- one normalized sentence''\n",
    "    :return: batches iterator\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip().split()\n",
    "\n",
    "        # lines with less than `minlength` words are omitted\n",
    "        if len(line) >= minlength:\n",
    "            batch.append(line)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(path, batch_size=50, minlength=5):\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "\n",
    "        if len(line) >= minlength:\n",
    "            batch_count += 1\n",
    "            if batch_count >= batch_size:\n",
    "                count += 1\n",
    "                batch_count = 0\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vectors(text, w2v_model, maxlen, vocabulary):\n",
    "    \"\"\"\n",
    "        Token sequence -- to a list of word vectors;\n",
    "        if token not in vocabulary, it is skipped; the rest of\n",
    "        the slots up to `maxlen` are replaced with zeroes\n",
    "    :param text: list of tokens\n",
    "    :param w2v_model: gensim w2v model\n",
    "    :param maxlen: max. length of the sentence; the rest is just cut away\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    acc_vecs = []\n",
    "\n",
    "    for word in text:\n",
    "        if word in w2v_model and (vocabulary is None or word in vocabulary):\n",
    "            acc_vecs.append(w2v_model.wv[word])\n",
    "\n",
    "    # padding for consistent length with ZERO vectors\n",
    "    if len(acc_vecs) < maxlen:\n",
    "        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n",
    "\n",
    "    return acc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_tensors(\n",
    "    path, \n",
    "    batch_size=50, \n",
    "    vocabulary=None,\n",
    "    maxlen=100, \n",
    "    pad_value=0, \n",
    "    minsentlength=5,\n",
    "    w2v_model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "        Data for training the NN -- from text file to word vectors sequences batches\n",
    "    :param path:\n",
    "    :param batch_size:\n",
    "    :param vocabulary:\n",
    "    :param maxlen:\n",
    "    :param pad_value:\n",
    "    :param minsentlength:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for batch in read_data_batches(path, batch_size, minsentlength):\n",
    "        batch_vecs = []\n",
    "        batch_texts = []\n",
    "\n",
    "        for text in batch:\n",
    "            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n",
    "            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        yield np.stack(batch_vecs, axis=0), batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"\n",
    "        Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for ABAE aspects matrix initialization\n",
    "    \"\"\"\n",
    "\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output(input_, kernel, padding, stride):\n",
    "    \"\"\"Calculate the Output size in Convolution layer\n",
    "    \n",
    "    \"\"\"\n",
    "    return ((input_ - kernel + 2 * padding) / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = F.relu(self.conv(x))\n",
    "        print(f'conv layer shape {result.shape}')\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=3, stride=2):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=0\n",
    "            ) \n",
    "            for _ in range(num_capsules)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), 32 * 96 , -1)\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 96, in_channels=8, out_channels=16):\n",
    "        super(SecondaryCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1)).to(args.device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        conv_in_ch,\n",
    "        conv_out_ch,\n",
    "        conv_kernel,\n",
    "        conv_stride,\n",
    "        \n",
    "        prime_num_capsules,\n",
    "        prime_out_ch,\n",
    "        prime_kernel,\n",
    "        prime_stride,\n",
    "    ):\n",
    "        super(CapsNet, self).__init__()\n",
    "        \n",
    "        self.conv_layer = ConvLayer(\n",
    "            in_channels=conv_in_ch,\n",
    "            out_channels=conv_out_ch,\n",
    "            kernel_size=conv_kernel,\n",
    "            stride=conv_stride,\n",
    "        )\n",
    "        \n",
    "        self.primary_caps = PrimaryCaps(\n",
    "            num_capsules=prime_num_capsules, \n",
    "            in_channels=conv_out_ch, \n",
    "            out_channels=prime_out_ch, \n",
    "            kernel_size=prime_kernel, \n",
    "            stride=prime_stride,\n",
    "        )\n",
    "        \n",
    "        self.secondary_caps = SecondaryCaps()\n",
    "        self.fc = nn.Linear(16 * 10, 201)\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.secondary_caps(self.primary_caps(self.conv_layer(data)))\n",
    "        output = output.reshape(50, 160)\n",
    "\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim: int = 200, \n",
    "        asp_count: int = 30,\n",
    "        ortho_reg: float = 0.1, \n",
    "        maxlen: int = 201, \n",
    "        init_aspects_matrix=None,\n",
    "        \n",
    "        cn_conv_out_ch=256,\n",
    "        cn_conv_kernel=9,\n",
    "        cn_conv_stride=1,\n",
    "        cn_prime_num_capsules=8,\n",
    "        cn_prime_out_ch=32,\n",
    "        cn_prime_kernel=3,\n",
    "        cn_prime_stride=2,\n",
    "    ):\n",
    "        super(CBAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.ortho = ortho_reg\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.caps_net = CapsNet(\n",
    "            conv_in_ch=wv_dim,\n",
    "            conv_out_ch=cn_conv_out_ch,\n",
    "            conv_kernel=cn_conv_kernel,\n",
    "            conv_stride=cn_conv_stride,\n",
    "            prime_num_capsules=cn_prime_num_capsules,\n",
    "            prime_out_ch=cn_prime_out_ch,\n",
    "            prime_kernel=cn_prime_kernel,\n",
    "            prime_stride=cn_prime_stride,\n",
    "        )\n",
    "        \n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax()\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "\n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Takes embeddings of a sentence as input, returns attention weights\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute attention scores, looking at text embeddings average\n",
    "        caps_weights = self.caps_net(text_embeddings.permute(0, 2, 1))\n",
    "\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(caps_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return caps_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "\n",
    "        # negative samples are averaged\n",
    "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
    "        \n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "\n",
    "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "\n",
    "        # loss\n",
    "        reconstruction_triplet_loss = CBAE._reconstruction_loss(\n",
    "            weighted_text_emb,\n",
    "            recovered_emb,\n",
    "            averaged_negative_samples,\n",
    "        )\n",
    "        \n",
    "        max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
    "        reconstruction_triplet_loss\n",
    "\n",
    "        return self.ortho * self._ortho_regularizer() + max_margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
    "\n",
    "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
    "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
    "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
    "\n",
    "        return reconstruction_triplet_loss\n",
    "\n",
    "    def _ortho_regularizer(self):\n",
    "        return torch.norm(\n",
    "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
    "            - torch.eye(self.asp_count).to(args.device))\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=15):\n",
    "        words = []\n",
    "\n",
    "        # getting aspects embeddings\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "\n",
    "        # getting scalar products of word embeddings and aspect embeddings;\n",
    "        # to obtain the ``probabilities'', one should also apply softmax\n",
    "        words_scores = w2v_model.wv.syn0.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data_json='Electronics_5.json',\n",
    "    \n",
    "    w2v_file='Electronics_5.w2v',\n",
    "    w2v_size=200,\n",
    "    w2v_window=5,\n",
    "    w2v_min_count=5,\n",
    "    w2v_workers=7,\n",
    "    w2v_sg=1,\n",
    "    w2v_negative=5,\n",
    "    w2v_iter=1,\n",
    "    w2v_max_vocab_size=20000,\n",
    "    \n",
    "    batch_size=50,\n",
    "    aspects_number=40,\n",
    "    ortho_reg=0.1,\n",
    "    epochs=1,\n",
    "    optimizer='adam',\n",
    "    neg_samples=5,\n",
    "    maxlen=201,\n",
    "    \n",
    "    cn_conv_out_channels = 256,\n",
    "    cn_conv_kernel = 9,\n",
    "    cn_conv_stride = 1,\n",
    "    cn_prime_num_capsules=8,\n",
    "    cn_prime_out_channels=32,\n",
    "    cn_prime_stride=2,\n",
    "    \n",
    "    cuda=True,\n",
    "    reload_from_files=True,\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading vectorizer\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    sentences = Sentences(args.data_json)\n",
    "    w2v = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        size=args.w2v_size, \n",
    "        window=args.w2v_window, \n",
    "        min_count=args.w2v_min_count, \n",
    "        workers=args.w2v_workers, \n",
    "        sg=args.w2v_sg,\n",
    "        negative=args.w2v_negative, \n",
    "        iter=args.w2v_iter, \n",
    "        max_vocab_size=args.w2v_max_vocab_size,\n",
    "    )\n",
    "    w2v.save(args.w2v_file)\n",
    "    print(f'{args.w2v_file} saved')\n",
    "    \n",
    "vectorizer = gensim.models.Word2Vec.load(args.w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ['she', 'He', 'his', 'She', 'son', 'husband', 'him', 'dad', 'daughter', 'wife']\n",
      "love ['LOVE', 'Love', 'loved', '\"Love', 'enjoy', 'hate', 'loves', 'appreciate', 'enjoyed', 'like']\n",
      "looks ['feels', 'Looks', 'look', 'looked', 'sleek', 'sounds', 'matches', 'finish', 'appearance', 'look.']\n",
      "buy ['purchase', 'buying', 'purchasing', 'sell', 'ordering', 'buy,', 'invest', 'try', 'buy.', 'order']\n",
      "laptop ['notebook', 'netbook', 'computer', 'laptop,', 'machine', 'laptop.', 'PC', 'desktop', 'tablet', 'pc']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"he\", \"love\", \"looks\", \"buy\", \"laptop\"]:\n",
    "    if word in vectorizer.wv.vocab:\n",
    "        print(word, [w for w, c in vectorizer.wv.similar_by_word(word=word)])\n",
    "    else:\n",
    "        print(word, \"not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = vectorizer.vector_size\n",
    "y = torch.zeros(args.batch_size, 1).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBAE(\n",
       "  (caps_net): CapsNet(\n",
       "    (conv_layer): ConvLayer(\n",
       "      (conv): Conv1d(200, 256, kernel_size=(9,), stride=(1,))\n",
       "    )\n",
       "    (primary_caps): PrimaryCaps(\n",
       "      (capsules): ModuleList(\n",
       "        (0): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (1): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (2): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (3): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (4): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (5): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (6): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "        (7): Conv1d(256, 32, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "    (secondary_caps): SecondaryCaps()\n",
       "    (fc): Linear(in_features=160, out_features=201, bias=True)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=200, out_features=40, bias=True)\n",
       "  (softmax_aspects): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBAE(\n",
    "    wv_dim=wv_dim,\n",
    "    asp_count=args.aspects_number,\n",
    "    init_aspects_matrix=get_centroids(vectorizer, aspects_count=args.aspects_number)\n",
    ")\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f758655c876845eaad40ae99d1d5b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=1.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebe6ba20f654f088d5683386d474952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=33782.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "conv layer shape torch.Size([50, 256, 193])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-9765a737981f>:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v_model and (vocabulary is None or word in vocabulary):\n",
      "<ipython-input-11-89ca3876819d>:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c_ij = F.softmax(b_ij)\n",
      "<ipython-input-13-fd25fcc264c8>:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aspects_importances = self.softmax_aspects(raw_importances)\n",
      "/home/kkrasikov/PycharmProjects/TopicModelingWithCapsNet/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches, and LR: 0.001\n",
      "1 recognized popped inserted hooked loaded opened plugged pops restart shut hooking unplug downloaded opens turned\n",
      "2 sturdy, sturdy. sturdy stylish durable. attractive plastic, constructed sleek durable lightweight, solidly comfortable. elegant padded\n",
      "3 80 40 25 200 100 20 50 1.5 1000 30 300 45 ten 15 60\n",
      "4 \"You Stars\", \"Fits \"very \"Does \"works \"They \"These \"it \"If \"What \"There \"Amazon \"Did \"Works\n",
      "5 contacted refund sent told died refused defective ago asked received shipped wrote \"B003ES5ZUU\", ship returned\n",
      "6 \"B003ES5ZUU\", \"B003ELYQGG\", \"B002WE6D44\", \"B002V88HFE\", \"William \"S. \"B007WTAJTO\", \"N. \"W. \"T. \"M. \"asin\": \"J. waste \"E.\n",
      "7 (you mode, button, icon it.The pressing side, (it whereas wall, select something, mode. area, allowing\n",
      "8 Also, Finally, However, Still, fact, Although Since Unfortunately, Unlike addition, Either Personally, Perhaps Now, While\n",
      "9 ear. ears. plugs. outlet. head. plug. side. end. hand. connector. wall. hands. other. pocket. place.\n",
      "10 pulling pressing turning touching hitting removing sliding plugging losing dropping picking hooking falling wasting pushing\n",
      "11 Kit Adapter Series Portable Lens Price\", Desktop Optical MP Wireless Drive Digital Speaker EF Case\n",
      "12 outlet. shots. choice. results. hub. reception. option. photos. speakers. lens. system. antenna. zoom. receiver. model.\n",
      "13 since. with. try. together. owned. soon. properly. available. included. for. through. else. down. before. about.\n",
      "14 Easy Worked Work Works Used Fits Plug Price\", Fast Simple Took Looks Lots Nice Read\n",
      "15 website, website password website. site, CD. address download site e-mail site. page downloaded account DHCP\n",
      "16 TO WAS HAVE MY WITH FOR OF AS YOU THE BE ARE YOUR IN ON\n",
      "17 protection, fit, protection. protection feel, value, color, weight, picture, cover, sound, design, quality, job\", \"B003ES5ZUU\",\n",
      "18 button, buttons, mute controls, dial control, equalizer menu buttons controls volume, button buttons. controls. select\n",
      "19 anyone.\", 2012\"} should\", 2014\"} needed\", case\", for.\", expected\", 2011\"} 2013\"} price\", camera\", 2010\"} bag\", job\",\n",
      "20 [13, [11, [14, [17, [15, [16, [19, [12, 16], 17], [10, 12], [9, 15], [20,\n",
      "21 f/2.8 focal aperture telephoto Nikkor 18-55mm 35mm 50mm lens. EF 70-200 lenses lenses. bokeh VR\n",
      "22 bed night, night. morning night chair kitchen outdoors sun day, sunlight yard day. dark sitting\n",
      "23 $150 $100 $200 $50 $40 $500 $300 dollars $20 $25 $30 $10 bucks $15 dollars.\n",
      "24 warranty. warranty, warranty lifetime period. service. dollars. shipping. support. rebate replacement. refund ago. year. months.\n",
      "25 \"08 \"09 \"12 \"10 \"11 \"05 \"03 \"04 \"06 \"07 \"01 \"02 2012\"} 2010\"} 2014\"}\n",
      "26 \"Kenneth \"Jonathan \"William \"Kevin \"Bruce \"Stephen \"Thomas \"Peter \"Richard \"Scott \"N. \"Jerry \"Eric \"Robert \"Daniel\n",
      "27 mids treble reproduction highs lows midrange crisp, distortion fidelity bass bass, muddy bass. distortion. clarity\n",
      "28 tighten folds legs attaches tension rotate fold friction flap bend screw securely sliding folded slide\n",
      "29 par comparable identical equivalent models, comparing differences equal compared weighs models. comparison same, superior whereas\n",
      "30 cloth flap padding velcro dust zipper pockets rubber adhesive scratches edges cups foam strap straps\n",
      "31 port, Ethernet 3.5mm ethernet DVI PCI ports, RCA port port. HDMI 3.0 VGA coaxial ports\n",
      "32 library documents music, movies, classical surf browse books, listen music trips games, files school files,\n",
      "33 primarily mainly mostly movies, video, books, occasional rarely games, music, regularly browsing occasionally outdoors casual\n",
      "34 19, 22, 27, 18, 29, 21, 26, 28, 23, 25, 20, 17, 24, 16, 14,\n",
      "35 brands chargers headsets players, routers players mice keyboards systems, systems protectors cases models, formats models\n",
      "36 been \"I've gotten gone owned enjoyed served lasted lived years, spent since. taken \"Have performed\n",
      "37 weeks, weeks. months, months. months weeks month month, month. week hours. week, hours, minutes, minutes.\n",
      "38 reviews reviews, comments reviewers reviews. reviewer experiences review, positive negative posted agree complained review complaining\n",
      "39 skeptical worried hesitant concerned excited amazed admit impressed surprised complaining told wondering convinced afraid complained\n",
      "40 apartment yard rooms room, floor basement house, feet wall, upstairs miles chair garage ceiling kitchen\n",
      "Loss: 95354.640625\n",
      "\n",
      "conv layer shape torch.Size([50, 256, 193])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-fd25fcc264c8>:112: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  words_scores = w2v_model.wv.syn0.dot(aspects)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n",
      "conv layer shape torch.Size([50, 256, 193])\n"
     ]
    }
   ],
   "source": [
    "epoch_bar = tqdm(\n",
    "    desc='training routine', \n",
    "    total=args.epochs,\n",
    "    position=0\n",
    ")\n",
    "\n",
    "train_bar = tqdm(\n",
    "    desc='train',\n",
    "    total=get_num_batches(args.data_json, args.batch_size, args.maxlen), \n",
    "    position=1, \n",
    "    leave=True\n",
    ")\n",
    "\n",
    "\n",
    "for t in range(args.epochs):\n",
    "\n",
    "    print(\"Epoch %d/%d\" % (t + 1, args.epochs))\n",
    "\n",
    "    data_iterator = read_data_tensors(\n",
    "        args.data_json,\n",
    "        batch_size=args.batch_size, \n",
    "        maxlen=args.maxlen,\n",
    "        w2v_model=vectorizer,\n",
    "    )\n",
    "\n",
    "    for item_number, (x, texts) in enumerate(data_iterator):\n",
    "        if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
    "            x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
    "\n",
    "        x = torch.from_numpy(x).to(args.device)\n",
    "\n",
    "        # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
    "        negative_samples = torch.stack(\n",
    "            tuple([x[torch.randperm(x.shape[0])[:args.neg_samples]] \n",
    "                   for _ in range(args.batch_size)])\n",
    "        ).to(args.device)\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model(x, negative_samples)\n",
    "\n",
    "        # error computation\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if item_number % 1000 == 0:\n",
    "\n",
    "            print(item_number, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            for i, aspect in enumerate(model.get_aspect_words(vectorizer)):\n",
    "                print(i + 1, \" \".join([a for a in aspect]))\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print()\n",
    "\n",
    "        train_bar.update()\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
