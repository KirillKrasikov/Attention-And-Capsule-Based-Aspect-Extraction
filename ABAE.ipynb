{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkrasikov/PycharmProjects/TopicModelingWithCapsNet/venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "from sklearn.cluster.k_means_ import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.num_lines = sum(1 for line in open(filename))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in tqdm(\n",
    "            codecs.open(self.filename, \"r\", encoding=\"utf-8\"), \n",
    "            self.filename, \n",
    "            self.num_lines\n",
    "        ):\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_batches(path, batch_size=50, minlength=5):\n",
    "    \"\"\"\n",
    "        Reading batched texts of given min. length\n",
    "    :param path: path to the text file ``one line -- one normalized sentence''\n",
    "    :return: batches iterator\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        line = line.strip().split()\n",
    "\n",
    "        # lines with less than `minlength` words are omitted\n",
    "        if len(line) >= minlength:\n",
    "            batch.append(line)\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(path, batch_size=50, minlength=5):\n",
    "    count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "\n",
    "        if len(line) >= minlength:\n",
    "            batch_count += 1\n",
    "            if batch_count >= batch_size:\n",
    "                count += 1\n",
    "                batch_count = 0\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vectors(text, w2v_model, maxlen, vocabulary):\n",
    "    \"\"\"\n",
    "        Token sequence -- to a list of word vectors;\n",
    "        if token not in vocabulary, it is skipped; the rest of\n",
    "        the slots up to `maxlen` are replaced with zeroes\n",
    "    :param text: list of tokens\n",
    "    :param w2v_model: gensim w2v model\n",
    "    :param maxlen: max. length of the sentence; the rest is just cut away\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    acc_vecs = []\n",
    "\n",
    "    for word in text:\n",
    "        if word in w2v_model and (vocabulary is None or word in vocabulary):\n",
    "            acc_vecs.append(w2v_model.wv[word])\n",
    "\n",
    "    # padding for consistent length with ZERO vectors\n",
    "    if len(acc_vecs) < maxlen:\n",
    "        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n",
    "\n",
    "    return acc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_tensors(\n",
    "    path, \n",
    "    batch_size=50, \n",
    "    vocabulary=None,\n",
    "    maxlen=100, \n",
    "    pad_value=0, \n",
    "    minsentlength=5,\n",
    "    w2v_model=None,\n",
    "):\n",
    "    \"\"\"\n",
    "        Data for training the NN -- from text file to word vectors sequences batches\n",
    "    :param path:\n",
    "    :param batch_size:\n",
    "    :param vocabulary:\n",
    "    :param maxlen:\n",
    "    :param pad_value:\n",
    "    :param minsentlength:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for batch in read_data_batches(path, batch_size, minsentlength):\n",
    "        batch_vecs = []\n",
    "        batch_texts = []\n",
    "\n",
    "        for text in batch:\n",
    "            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n",
    "            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        yield np.stack(batch_vecs, axis=0), batch_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(w2v_model, aspects_count):\n",
    "    \"\"\"\n",
    "        Clustering all word vectors with K-means and returning L2-normalizes\n",
    "        cluster centroids; used for ABAE aspects matrix initialization\n",
    "    \"\"\"\n",
    "\n",
    "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
    "    m = []\n",
    "\n",
    "    for k in w2v_model.wv.vocab:\n",
    "        m.append(w2v_model.wv[k])\n",
    "\n",
    "    m = np.matrix(m)\n",
    "\n",
    "    km.fit(m)\n",
    "    clusters = km.cluster_centers_\n",
    "\n",
    "    # L2 normalization\n",
    "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "\n",
    "    return norm_aspect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, wv_dim: int, maxlen: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "\n",
    "        # max sentence length -- batch 2nd dim size\n",
    "        self.maxlen = maxlen\n",
    "        self.M = Parameter(torch.empty(size=(wv_dim, wv_dim)))\n",
    "        init.kaiming_uniform(self.M.data)\n",
    "\n",
    "        # softmax for attending to wod vectors\n",
    "        self.attention_softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        # (b, wv, 1)\n",
    "        \n",
    "        input_embeddings = input_embeddings.to(args.device)\n",
    "        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2).to(args.device)\n",
    "\n",
    "        # (wv, wv) x (b, wv, 1) -> (b, wv, 1)\n",
    "        product_1 = torch.matmul(self.M, mean_embedding)\n",
    "\n",
    "        # (b, maxlen, wv) x (b, wv, 1) -> (b, maxlen, 1)\n",
    "        product_2 = torch.matmul(input_embeddings, product_1).squeeze(2)\n",
    "\n",
    "        results = self.attention_softmax(product_2)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'wv_dim={self.wv_dim}, maxlen={self.maxlen}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n",
    "        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n",
    "        https://aclweb.org/anthology/papers/P/P17/P17-1036/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        wv_dim: int = 200, \n",
    "        asp_count: int = 30,\n",
    "        ortho_reg: float = 0.1, \n",
    "        maxlen: int = 201, \n",
    "        init_aspects_matrix=None\n",
    "    ):\n",
    "        \"\"\"Initializing the model\n",
    "        \n",
    "        :param wv_dim: word vector size\n",
    "        :param asp_count: number of aspects\n",
    "        :param ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
    "        :param maxlen: sentence max length taken into account\n",
    "        :param init_aspects_matrix: None or init. matrix for aspects\n",
    "        \"\"\"\n",
    "        super(ABAE, self).__init__()\n",
    "        self.wv_dim = wv_dim\n",
    "        self.asp_count = asp_count\n",
    "        self.ortho = ortho_reg\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.attention = SelfAttention(wv_dim, maxlen)\n",
    "        self.linear_transform = torch.nn.Linear(self.wv_dim, self.asp_count)\n",
    "        self.softmax_aspects = torch.nn.Softmax()\n",
    "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
    "\n",
    "        if init_aspects_matrix is None:\n",
    "            torch.nn.init.xavier_uniform(self.aspects_embeddings)\n",
    "        else:\n",
    "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T)\n",
    "\n",
    "    def get_aspects_importances(self, text_embeddings):\n",
    "        \"\"\"Takes embeddings of a sentence as input, returns attention weights\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute attention scores, looking at text embeddings average\n",
    "        attention_weights = self.attention(text_embeddings)\n",
    "\n",
    "        # multiplying text embeddings by attention scores -- and summing\n",
    "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
    "        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
    "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
    "                                         ).squeeze()\n",
    "\n",
    "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
    "        raw_importances = self.linear_transform(weighted_text_emb)\n",
    "\n",
    "        # computing 'aspects distribution in a sentence'\n",
    "        aspects_importances = self.softmax_aspects(raw_importances)\n",
    "\n",
    "        return attention_weights, aspects_importances, weighted_text_emb\n",
    "\n",
    "    def forward(self, text_embeddings, negative_samples_texts):\n",
    "\n",
    "        # negative samples are averaged\n",
    "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
    "\n",
    "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
    "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
    "\n",
    "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
    "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
    "        print(f'recovered_emb: {recovered_emb.shape}')\n",
    "\n",
    "        # loss\n",
    "        reconstruction_triplet_loss = ABAE._reconstruction_loss(\n",
    "            weighted_text_emb,\n",
    "            recovered_emb,\n",
    "            averaged_negative_samples,\n",
    "        )\n",
    "        print(f'reconstruction_triplet_loss: {reconstruction_triplet_loss.shape}')\n",
    "        \n",
    "        max_margin = torch.max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss))\n",
    "        reconstruction_triplet_loss\n",
    "\n",
    "        return self.ortho * self._ortho_regularizer() + max_margin\n",
    "\n",
    "    @staticmethod\n",
    "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
    "\n",
    "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
    "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
    "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1) + negative_dot_products, dim=1)\n",
    "\n",
    "        return reconstruction_triplet_loss\n",
    "\n",
    "    def _ortho_regularizer(self):\n",
    "        return torch.norm(\n",
    "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
    "            - torch.eye(self.asp_count).to(args.device))\n",
    "\n",
    "    def get_aspect_words(self, w2v_model, topn=15):\n",
    "        words = []\n",
    "\n",
    "        # getting aspects embeddings\n",
    "        aspects = self.aspects_embeddings.cpu().detach().numpy()\n",
    "\n",
    "        # getting scalar products of word embeddings and aspect embeddings;\n",
    "        # to obtain the ``probabilities'', one should also apply softmax\n",
    "        words_scores = w2v_model.wv.syn0.dot(aspects)\n",
    "\n",
    "        for row in range(aspects.shape[1]):\n",
    "            argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]\n",
    "            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])\n",
    "            words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data_json='Electronics_5.json',\n",
    "    \n",
    "    w2v_file='Electronics_5.w2v',\n",
    "    w2v_size=200,\n",
    "    w2v_window=5,\n",
    "    w2v_min_count=5,\n",
    "    w2v_workers=7,\n",
    "    w2v_sg=1,\n",
    "    w2v_negative=5,\n",
    "    w2v_iter=1,\n",
    "    w2v_max_vocab_size=20000,\n",
    "    \n",
    "    batch_size=50,\n",
    "    aspects_number=40,\n",
    "    ortho_reg=0.1,\n",
    "    epochs=1,\n",
    "    optimizer='adam',\n",
    "    neg_samples=5,\n",
    "    maxlen=201,\n",
    "    \n",
    "    cuda=True,\n",
    "    reload_from_files=True,\n",
    ")\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading vectorizer\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    sentences = Sentences(args.data_json)\n",
    "    w2v = gensim.models.Word2Vec(\n",
    "        sentences, \n",
    "        size=args.w2v_size, \n",
    "        window=args.w2v_window, \n",
    "        min_count=args.w2v_min_count, \n",
    "        workers=args.w2v_workers, \n",
    "        sg=args.w2v_sg,\n",
    "        negative=args.w2v_negative, \n",
    "        iter=args.w2v_iter, \n",
    "        max_vocab_size=args.w2v_max_vocab_size,\n",
    "    )\n",
    "    w2v.save(args.w2v_file)\n",
    "    print(f'{args.w2v_file} saved')\n",
    "    \n",
    "vectorizer = gensim.models.Word2Vec.load(args.w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ['she', 'He', 'his', 'She', 'son', 'husband', 'him', 'dad', 'daughter', 'wife']\n",
      "love ['LOVE', 'Love', 'loved', '\"Love', 'enjoy', 'hate', 'loves', 'appreciate', 'enjoyed', 'like']\n",
      "looks ['feels', 'Looks', 'look', 'looked', 'sleek', 'sounds', 'matches', 'finish', 'appearance', 'look.']\n",
      "buy ['purchase', 'buying', 'purchasing', 'sell', 'ordering', 'buy,', 'invest', 'try', 'buy.', 'order']\n",
      "laptop ['notebook', 'netbook', 'computer', 'laptop,', 'machine', 'laptop.', 'PC', 'desktop', 'tablet', 'pc']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"he\", \"love\", \"looks\", \"buy\", \"laptop\"]:\n",
    "    if word in vectorizer.wv.vocab:\n",
    "        print(word, [w for w, c in vectorizer.wv.similar_by_word(word=word)])\n",
    "    else:\n",
    "        print(word, \"not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = vectorizer.vector_size\n",
    "y = torch.zeros(args.batch_size, 1).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-29ddc00b0ff3>:9: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(self.M.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ABAE(\n",
       "  (attention): SelfAttention(\n",
       "    wv_dim=200, maxlen=201\n",
       "    (attention_softmax): Softmax(dim=None)\n",
       "  )\n",
       "  (linear_transform): Linear(in_features=200, out_features=40, bias=True)\n",
       "  (softmax_aspects): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ABAE(\n",
    "    wv_dim=wv_dim,\n",
    "    asp_count=args.aspects_number,\n",
    "    init_aspects_matrix=get_centroids(vectorizer, aspects_count=args.aspects_number)\n",
    ")\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589522ba373b4b8291aca55f46dc35ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=1.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2e23a31316407cb898d892f2427411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=33782.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "0 batches, and LR: 0.001\n",
      "1 25 40 80 20 30 50 200 45 100 15 60 1000 18 12 1.5\n",
      "2 comments reviews reviews, reviewers questions reviews. posted review, written complained complaining comment reviewer agree experiences\n",
      "3 Phillips Philips Miller\", \"Kenneth \"Bruce \"Bill Johnson\", \"Stephen \"B007WTAJTO\", \"Dr. \"Jerry M. \"L. Smith\", \"Chris\n",
      "4 sturdy, sturdy. sturdy bulky, durable. flimsy, lightweight, lightweight durable, compact, stylish comfortable, heavy, plastic, durable\n",
      "5 GB 64 PCI Drive SATA 3.0 SDHC Card MB Desktop Vista XP Flash 2.0 Gigabit\n",
      "6 \"B003ES5ZUU\", \"B002WE6D44\", \"B003ELYQGG\", mode, determine (you \"B002V88HFE\", annoying, suspect it.The issue, enables means often, causes\n",
      "7 Stars\", \"Amazon \"very \"it \"Five \"works \"You \"There \"Does \"Did \"these \"good \"If \"Exactly \"They\n",
      "8 impressed skeptical excited amazed hesitant surprised pleased disappointed pleasantly thrilled shocked frustrated concerned worried satisfied\n",
      "9 TO WAS MY HAVE FOR WITH OF AS THE YOU BE YOUR IN ARE ON\n",
      "10 \"B0019EHU8G\", \"B002V88HFE\", \"B003ES5ZUU\", \"B002WE6D44\", \"Bruce \"B003ELYQGG\", \"Jerry \"Jonathan Johnson\", \"Pros: \"B000QUUFRW\", \"Kenneth Miller\", \"Thomas \"B000LRMS66\",\n",
      "11 f/2.8 telephoto EF 18-55mm focal lenses 70-200 Nikkor lenses. 50mm 35mm 70-200mm aperture lens. lens,\n",
      "12 waited wait waiting morning searched seconds seconds, ago refund later, later ago, minutes later. minutes,\n",
      "13 \"B003ES5ZUU\", \"I've contacted \"B002WE6D44\", ago \"B003ELYQGG\", \"Bruce \"B002V88HFE\", died told recently \"William refused \"L. \"After\n",
      "14 3.5mm RCA port, coaxial jack male jacks outlet ethernet Ethernet port plug. DVI DC plugs\n",
      "15 since. try. outlet. with. stores. together. site. other. option. owned. itself. else. choice. mode. past.\n",
      "16 [13, [11, [14, [17, [16, [15, [19, [12, [10, [9, 16], [20, 17], 12], [18,\n",
      "17 menu select button, dial navigate menus scroll mute password buttons, settings button icon equalizer controls,\n",
      "18 refund shipping shipping. warranty. warranty warranty, rebate ship contacted service. seller shipped delivery e-mail service\n",
      "19 TV, router, network, player, bedroom theater modem, VCR 5.1 basement upstairs tv A/V TV router\n",
      "20 documents files, files browse files. movies, videos formats photos, photos upload library surf movies DVDs\n",
      "21 scratches scratched dust wipe damage dirt scratch tear dirty falling slip protect crack wet cleaned\n",
      "22 sliding facing slide pops located pressing touching opposite lay sides sits flap folds edge corner\n",
      "23 loved liked enjoyed loves likes love daughter sister excited She skeptical birthday amazed enjoying thrilled\n",
      "24 \"Kenneth \"Jonathan \"Bruce \"William \"Kevin \"Peter \"Thomas \"Stephen \"Richard \"Scott \"N. \"Jerry \"Eric \"Daniel \"Robert\n",
      "25 tighten flap folds velcro legs folded strap friction tension Velcro fold attaches cups securely screws\n",
      "26 highs mids treble lows bass bass. bass, midrange muddy distortion hiss distortion. reproduction tinny crisp,\n",
      "27 trips town yard apartment rooms house, city house stations classical gym networks trip house. outdoors\n",
      "28 2010\"} 2012\"} 2011\"} 19, 27, 22, 21, 29, 18, 23, 30, 26, 28, 25, 31,\n",
      "29 plugging hooking plugged Plug plug connecting unplug recognized hooked inserted connected placing putting outlet plugs\n",
      "30 choice choice. choice, decision investment option. choices suited option compromise luck purchases experiences purchase. idea.\n",
      "31 Price\", Cable\", Quality\", Work Read Poor Value\", Product\", Great\", Easy Use \"B003ELYQGG\", Little \"W. Value\n",
      "32 space. space room. clutter room memory. pockets storage slot. documents slots bulk compartment files. valuable\n",
      "33 anyone.\", again.\", for.\", future.\", do.\", to.\", job.\", bag.\", go.\", recommend.\", needed.\", in.\", one.\", time.\", need.\",\n",
      "34 wider wide shorter sharper thicker width diameter heavier larger thinner brighter focal narrow smaller greater\n",
      "35 weeks, weeks. months, months. months weeks hours. hours, minutes. minutes, month minutes month, seconds, week\n",
      "36 buds cups ear earbuds cushions ear. ears. pads ears ears, earpieces earphones ear, headphones in-ear\n",
      "37 simply Simply directly literally Plug truly absolutely automatically instantly either just allow Once essentially obviously\n",
      "38 $100 dollars dollars. $150 $200 comparable costs $50 $500 weighs $40 par $300 bucks dollar\n",
      "39 this.\", these.\", it.\", this\", them.\", it\", it!\", one.\", that.\", else.\", item.\", for.\", unit.\", again.\", purchase.\",\n",
      "40 fact, Finally, Also, Personally, Still, addition, However, Perhaps Unfortunately, Although Again, Since Now, Either First,\n",
      "Loss: 105607.3828125\n",
      "\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-9765a737981f>:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v_model and (vocabulary is None or word in vocabulary):\n",
      "<ipython-input-8-29ddc00b0ff3>:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  results = self.attention_softmax(product_2)\n",
      "<ipython-input-27-e13ff7f87bba>:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  aspects_importances = self.softmax_aspects(raw_importances)\n",
      "/home/kkrasikov/PycharmProjects/TopicModelingWithCapsNet/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([50, 1])) that is different to the input size (torch.Size([50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "<ipython-input-27-e13ff7f87bba>:113: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  words_scores = w2v_model.wv.syn0.dot(aspects)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n",
      "attention weights: torch.Size([50, 201])\n",
      "text embeddings: torch.Size([50, 201, 200])\n",
      "aspects importances: torch.Size([50, 40])\n",
      "weighted_text_emb: torch.Size([50, 200])\n",
      "recovered_emb: torch.Size([50, 200])\n",
      "reconstruction_triplet_loss: torch.Size([50])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-040333b419d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pad with 0 if smaller than batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a0da3bc61094>\u001b[0m in \u001b[0;36mread_data_tensors\u001b[0;34m(path, batch_size, vocabulary, maxlen, pad_value, minsentlength, w2v_model)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mvectors_as_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext2vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mbatch_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_as_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mbatch_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9765a737981f>\u001b[0m in \u001b[0;36mtext2vectors\u001b[0;34m(text, w2v_model, maxlen, vocabulary)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0macc_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/TopicModelingWithCapsNet/venv/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m                 warnings.warn(\n\u001b[0m\u001b[1;32m   1457\u001b[0m                     \u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m                     \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_bar = tqdm(\n",
    "    desc='training routine', \n",
    "    total=args.epochs,\n",
    "    position=0\n",
    ")\n",
    "\n",
    "train_bar = tqdm(\n",
    "    desc='train',\n",
    "    total=get_num_batches(args.data_json, args.batch_size, args.maxlen), \n",
    "    position=1, \n",
    "    leave=True\n",
    ")\n",
    "\n",
    "\n",
    "for t in range(args.epochs):\n",
    "\n",
    "    print(\"Epoch %d/%d\" % (t + 1, args.epochs))\n",
    "\n",
    "    data_iterator = read_data_tensors(\n",
    "        args.data_json,\n",
    "        batch_size=args.batch_size, \n",
    "        maxlen=args.maxlen,\n",
    "        w2v_model=vectorizer,\n",
    "    )\n",
    "\n",
    "    for item_number, (x, texts) in enumerate(data_iterator):\n",
    "        if x.shape[0] < args.batch_size:  # pad with 0 if smaller than batch size\n",
    "            x = np.pad(x, ((0, args.batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
    "\n",
    "        x = torch.from_numpy(x).to(args.device)\n",
    "\n",
    "        # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
    "        negative_samples = torch.stack(\n",
    "            tuple([x[torch.randperm(x.shape[0])[:args.neg_samples]] \n",
    "                   for _ in range(args.batch_size)])\n",
    "        ).to(args.device)\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model(x, negative_samples)\n",
    "\n",
    "        # error computation\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if item_number % 1000 == 0:\n",
    "\n",
    "            print(item_number, \"batches, and LR:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            for i, aspect in enumerate(model.get_aspect_words(vectorizer)):\n",
    "                print(i + 1, \" \".join([a for a in aspect]))\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print()\n",
    "\n",
    "        train_bar.update()\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
